{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adad1d9b-3645-4111-8a0b-a00c15b2eca2",
   "metadata": {},
   "source": [
    "# Fine-tune Llama 2 for chat & dialogue summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c292d278-46f2-45f4-91ee-66d996191b75",
   "metadata": {},
   "source": [
    "Welcome!\n",
    "\n",
    "Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters.\n",
    "\n",
    "In this Jupyter Notebook, we will learn how to fine-tune the meta-llama/Llama-2-7b-hf, the smallest of the collection, with 7B parameters, using AI Stduio Deep Learning workspace with GPU and MLFlow for tracking the metrics!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681de814-9166-4045-bcd6-17dc201434d2",
   "metadata": {},
   "source": [
    "## Setup Development Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63153c81-d596-49bf-b363-850732fea85b",
   "metadata": {},
   "source": [
    "Our first step is to install some extra required packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68477a05-3635-4c08-ad22-59776322d8b9",
   "metadata": {},
   "source": [
    "##### Installing extra libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e689650-3087-4ffa-bce5-4089af43fd6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (12.0.1)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.2.0,>=2023.1.0 (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting huggingface-hub>=0.19.4 (from datasets)\n",
      "  Downloading huggingface_hub-0.21.4-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m960.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m587.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading huggingface_hub-0.21.4-py3-none-any.whl (346 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.4/346.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m911.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.6/301.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, pyarrow-hotfix, multidict, fsspec, frozenlist, dill, async-timeout, yarl, multiprocess, huggingface-hub, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.7\n",
      "    Uninstalling dill-0.3.7:\n",
      "      Successfully uninstalled dill-0.3.7\n",
      "Successfully installed aiohttp-3.9.3 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.18.0 dill-0.3.8 frozenlist-1.4.1 fsspec-2024.2.0 huggingface-hub-0.21.4 multidict-6.0.5 multiprocess-0.70.16 pyarrow-hotfix-0.6 xxhash-3.4.1 yarl-1.9.4\n",
      "Collecting peft\n",
      "  Downloading peft-0.9.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (23.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.0.0+cu118)\n",
      "Collecting transformers (from peft)\n",
      "  Downloading transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.7/130.7 kB\u001b[0m \u001b[31m241.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.1)\n",
      "Collecting accelerate>=0.21.0 (from peft)\n",
      "  Downloading accelerate-0.27.2-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting safetensors (from peft)\n",
      "  Downloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.21.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.2.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.5.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.3)\n",
      "Requirement already satisfied: triton==2.0.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (2.0.0)\n",
      "Requirement already satisfied: cmake in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.13.0->peft) (3.25.0)\n",
      "Requirement already satisfied: lit in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.13.0->peft) (15.0.7)\n",
      "Collecting regex!=2019.12.17 (from transformers->peft)\n",
      "  Downloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m189.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers<0.19,>=0.14 (from transformers->peft)\n",
      "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "Downloading peft-0.9.0-py3-none-any.whl (190 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.9/190.9 kB\u001b[0m \u001b[31m785.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-0.27.2-py3-none-any.whl (279 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.38.2-py3-none-any.whl (8.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.0/774.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: safetensors, regex, tokenizers, transformers, accelerate, peft\n",
      "Successfully installed accelerate-0.27.2 peft-0.9.0 regex-2023.12.25 safetensors-0.4.2 tokenizers-0.15.2 transformers-4.38.2\n",
      "Collecting trl\n",
      "  Downloading trl-0.7.11-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from trl) (2.0.0+cu118)\n",
      "Requirement already satisfied: transformers>=4.31.0 in /opt/conda/lib/python3.10/site-packages (from trl) (4.38.2)\n",
      "Requirement already satisfied: numpy>=1.18.2 in /opt/conda/lib/python3.10/site-packages (from trl) (1.24.3)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from trl) (0.27.2)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from trl) (2.18.0)\n",
      "Collecting tyro>=0.5.11 (from trl)\n",
      "  Downloading tyro-0.7.3-py3-none-any.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (4.5.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.1.3)\n",
      "Requirement already satisfied: triton==2.0.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (2.0.0)\n",
      "Requirement already satisfied: cmake in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.4.0->trl) (3.25.0)\n",
      "Requirement already satisfied: lit in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.4.0->trl) (15.0.7)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (0.21.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (2023.12.25)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (4.66.1)\n",
      "Collecting docstring-parser>=0.14.1 (from tyro>=0.5.11->trl)\n",
      "  Downloading docstring_parser-0.15-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting rich>=11.1.0 (from tyro>=0.5.11->trl)\n",
      "  Downloading rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl)\n",
      "  Downloading shtab-1.7.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->trl) (5.9.8)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (12.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (2.2.0)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets->trl) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (3.9.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl) (2023.7.22)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=11.1.0->tyro>=0.5.11->trl)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.17.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->trl) (2.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2023.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl) (1.16.0)\n",
      "Downloading trl-0.7.11-py3-none-any.whl (155 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m307.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tyro-0.7.3-py3-none-any.whl (79 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m610.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
      "Downloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading shtab-1.7.0-py3-none-any.whl (14 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m180.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: shtab, mdurl, docstring-parser, markdown-it-py, rich, tyro, trl\n",
      "Successfully installed docstring-parser-0.15 markdown-it-py-3.0.0 mdurl-0.1.2 rich-13.7.1 shtab-1.7.0 trl-0.7.11 tyro-0.7.3\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.42.0-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.12.0)\n",
      "Requirement already satisfied: numpy<1.29.0,>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from scipy->bitsandbytes) (1.24.3)\n",
      "Downloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.42.0\n",
      "Collecting mlflow==2.11.0\n",
      "  Downloading mlflow-2.11.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: click<9,>=7.0 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.11.0) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle<4 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.11.0) (2.2.1)\n",
      "Requirement already satisfied: entrypoints<1 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.11.0) (0.4)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.11.0) (3.1.41)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.11.0) (6.0.1)\n",
      "Requirement already satisfied: protobuf<5,>=3.12.0 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.11.0) (4.25.2)\n",
      "Requirement already satisfied: pytz<2025 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.11.0) (2023.3.post1)\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.11.0) (2.31.0)\n",
      "Requirement already satisfied: packaging<24 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.11.0) (23.2)\n",
      "Requirement already satisfied: importlib-metadata!=4.7.0,<8,>=3.7.0 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.11.0) (6.11.0)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.11.0) (0.4.4)\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.11.0) (1.13.1)\n",
      "Requirement already satisfied: docker<8,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.11.0) (6.1.3)\n",
      "Requirement already satisfied: Flask<4 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.11.0) (2.3.3)\n",
      "Requirement already satisfied: numpy<2 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.11.0) (1.24.3)\n",
      "Requirement already satisfied: scipy<2 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.11.0) (1.12.0)\n",
      "Requirement already satisfied: pandas<3 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.11.0) (2.2.0)\n",
      "Requirement already satisfied: querystring-parser<2 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.11.0) (1.2.4)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.11.0) (2.0.25)\n",
      "Requirement already satisfied: scikit-learn<2 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.11.0) (1.4.0)\n",
      "Requirement already satisfied: pyarrow<16,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.11.0) (12.0.1)\n",
      "Requirement already satisfied: markdown<4,>=3.3 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.11.0) (3.5.2)\n",
      "Requirement already satisfied: matplotlib<4 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.11.0) (3.8.2)\n",
      "Collecting graphene<4 (from mlflow==2.11.0)\n",
      "  Downloading graphene-3.3-py2.py3-none-any.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: gunicorn<22 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.11.0) (21.2.0)\n",
      "Requirement already satisfied: Jinja2<4,>=2.11 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.11.0) (3.1.3)\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow==2.11.0) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4 in /opt/conda/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow==2.11.0) (4.5.0)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /opt/conda/lib/python3.10/site-packages (from docker<8,>=4.0.0->mlflow==2.11.0) (2.1.0)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from docker<8,>=4.0.0->mlflow==2.11.0) (1.7.0)\n",
      "Requirement already satisfied: Werkzeug>=2.3.7 in /opt/conda/lib/python3.10/site-packages (from Flask<4->mlflow==2.11.0) (3.0.1)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in /opt/conda/lib/python3.10/site-packages (from Flask<4->mlflow==2.11.0) (2.1.2)\n",
      "Requirement already satisfied: blinker>=1.6.2 in /opt/conda/lib/python3.10/site-packages (from Flask<4->mlflow==2.11.0) (1.7.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython<4,>=3.1.9->mlflow==2.11.0) (4.0.11)\n",
      "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow==2.11.0)\n",
      "  Downloading graphql_core-3.2.3-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow==2.11.0)\n",
      "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting aniso8601<10,>=8 (from graphene<4->mlflow==2.11.0)\n",
      "  Downloading aniso8601-9.0.1-py2.py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata!=4.7.0,<8,>=3.7.0->mlflow==2.11.0) (3.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2<4,>=2.11->mlflow==2.11.0) (2.1.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.11.0) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.11.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.11.0) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.11.0) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.11.0) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.11.0) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.11.0) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas<3->mlflow==2.11.0) (2023.4)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from querystring-parser<2->mlflow==2.11.0) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow==2.11.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow==2.11.0) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow==2.11.0) (2023.7.22)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn<2->mlflow==2.11.0) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn<2->mlflow==2.11.0) (3.2.0)\n",
      "Collecting typing-extensions>=4 (from alembic!=1.10.0,<2->mlflow==2.11.0)\n",
      "  Downloading typing_extensions-4.10.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy<3,>=1.4.0->mlflow==2.11.0) (3.0.3)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow==2.11.0) (5.0.0)\n",
      "Downloading mlflow-2.11.0-py3-none-any.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading graphene-3.3-py2.py3-none-any.whl (128 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m944.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m464.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading graphql_core-3.2.3-py3-none-any.whl (202 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.9/202.9 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading typing_extensions-4.10.0-py3-none-any.whl (33 kB)\n",
      "Installing collected packages: aniso8601, typing-extensions, graphql-core, graphql-relay, graphene, mlflow\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.5.0\n",
      "    Uninstalling typing_extensions-4.5.0:\n",
      "      Successfully uninstalled typing_extensions-4.5.0\n",
      "  Attempting uninstall: mlflow\n",
      "    Found existing installation: mlflow 2.6.0\n",
      "    Uninstalling mlflow-2.6.0:\n",
      "      Successfully uninstalled mlflow-2.6.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jupyter-server 2.12.5 requires jupyter-events>=0.9.0, but you have jupyter-events 0.6.3 which is incompatible.\n",
      "tensorflow 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed aniso8601-9.0.1 graphene-3.3 graphql-core-3.2.3 graphql-relay-3.2.0 mlflow-2.11.0 typing-extensions-4.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets # This one is for downloading our samsum dataset direclty from Hugging Face\n",
    "!pip install peft # Both peft and trl are the libs that help us \n",
    "!pip install trl # to configure our training methods and params\n",
    "!pip install bitsandbytes # This one will help us to quantize the model\n",
    "!pip install mlflow==2.11.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929a6f57-6753-4111-bf17-7233ea2994dd",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3343b68-5d60-4839-a4b8-3de8be2af2e5",
   "metadata": {},
   "source": [
    "The libraries used below are already installed by default inside our Deep Learning workspace (excpet for transformers, which was installed together with the extra libraries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc1fe6e3-0168-491a-b2bc-97934c755570",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-06 20:00:40.513805: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-06 20:00:41.252685: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from huggingface_hub import notebook_login\n",
    "from peft import LoraConfig, PeftModel, AutoPeftModelForCausalLM\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "import time\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4721a1b1-d378-4698-a68f-5d5f32ce637d",
   "metadata": {},
   "source": [
    "### Defining device and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00378e4f-7de0-4d38-86b3-39935b6ec439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'cuda:0' means that we want to use our GPU, if available. If not, uses CPU.\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Model name defines that we are using llama 2 with 7B parameters\n",
    "# MODEL_NAME = \"meta-llama/Llama-2-7b-hf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060dfdc2-1827-4661-8f94-3e1ac6bce9da",
   "metadata": {},
   "source": [
    "### Hugging Face token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00db2b54-1f31-4139-aafc-6efc96ec1f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08c3e01-399e-47c6-a5ab-e9207d3c4d11",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "216c35aa-1aaa-456e-9bb0-b043528d8382",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 6.06M/6.06M [00:02<00:00, 2.04MB/s]\n",
      "Downloading data: 100%|██████████| 347k/347k [00:00<00:00, 352kB/s]\n",
      "Downloading data: 100%|██████████| 335k/335k [00:00<00:00, 760kB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "214fcf1a1a5340609659ea1aad2feaac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/14732 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4905b7d80bb4c7d8e64b7a97d1b8744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b86f369af0e4006878af0770cbc1305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"samsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61bdc314-908a-4de2-b7e3-9f228439aa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dialogue(dialogue):\n",
    "    # Replace the '\\r\\n' with '\\n' to match the desired output format.\n",
    "    formatted_dialogue = re.sub(r'\\r\\n', '\\n', dialogue)\n",
    "    return formatted_dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b91cbefc-3260-4ed7-a109-3b821de2c1db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 14732\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 819\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 818\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e723539-6201-43fc-a05b-3aef4195c8d9",
   "metadata": {},
   "source": [
    "This dataset has 3 different divisions:\n",
    "- Train: has 14732 datapoints\n",
    "- Test: has 819 datapoints\n",
    "- Validation: has 818 datapoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95071c83-7908-4bf8-a487-32b64be8614e",
   "metadata": {},
   "source": [
    "Let's get a sample from the training set and see how it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3bc3de9-e633-415e-8e58-3dc890ddea38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '13818513',\n",
       " 'dialogue': \"Amanda: I baked  cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\",\n",
       " 'summary': 'Amanda baked cookies and will bring Jerry some tomorrow.'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703558a2-8be3-4e1c-80e3-50679f0ed55b",
   "metadata": {},
   "source": [
    "Well, each datapoint is a dict composed by 3 key-values pair:\n",
    "- The id of the conversation\n",
    "- The dialogue of the conversation\n",
    "- The summary of that dialogue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae808d8-3d08-4f27-9413-6452d120305f",
   "metadata": {},
   "source": [
    "Here's the dialogue from this first datapoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7d8ac70-8218-47fd-952b-0a0d21eb6803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Amanda: I baked  cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]['dialogue']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b3590b-4875-4d2a-848e-0262d4abd37b",
   "metadata": {},
   "source": [
    "We can see that we have some '\\r' between the user's name. We will have to clean up that Rs by replacing them for a '\\n', which means we will have a white space before the user's name, lke this:\n",
    "\n",
    "Amanda: I baked  cookies. Do you want some?\r\n",
    "Jerry: Sure!\r\n",
    "Amanda: I'll bring you tomorrow :-)\n",
    "\n",
    "Let's create a function th formats the dialogues for us.\n",
    "at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41733b28-5d7e-4aef-8921-bdbd1f184fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dialogue(dialogue):\n",
    "    # Replace the '\\r\\n' with '\\n' to match the desired output format.\n",
    "    formatted_dialogue = re.sub(r'\\r\\n', '\\n', dialogue)\n",
    "    return formatted_dialogue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402c772e-61e2-41f1-a4f3-4d04f922fa5c",
   "metadata": {},
   "source": [
    "Now let's test that funtion on the first train dialogue and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6d6e409-6e69-4e2e-9897-09e49776e3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amanda: I baked  cookies. Do you want some?\n",
      "Jerry: Sure!\n",
      "Amanda: I'll bring you tomorrow :-)\n"
     ]
    }
   ],
   "source": [
    "print(format_dialogue(dataset['train'][0]['dialogue']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f931601e-9ce2-42ad-9154-5c7c2e74f09a",
   "metadata": {},
   "source": [
    "Perfect! It works pretty well!\n",
    "\n",
    "Now it is time to build the input prompt for llama 2 model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16758c04-04f4-4c75-8766-6ad6222cff42",
   "metadata": {},
   "source": [
    "## Defining input prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da5223c1-b4dd-4816-bfd6-89f52af77b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\n",
    "Below is a conversation between friends in a chat. Write a summary of their conversation.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4cb27418-3815-4e8b-9490-c7748f54d9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_prompt(\n",
    "    conversation: str, summary: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT\n",
    ") -> str:\n",
    "    return f\"\"\"### Instruction: {system_prompt}\n",
    "\n",
    "### Input:\n",
    "{conversation.strip()}\n",
    "\n",
    "### Response:\n",
    "{summary}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0d85f3f-69e0-4de7-b202-000b2a26bebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(data_point):\n",
    "    summary = data_point['summary']\n",
    "    conversation_text = format_dialogue(data_point['dialogue'])\n",
    "    return {\n",
    "        \"conversation\": conversation_text,\n",
    "        \"summary\": summary,\n",
    "        \"text\": generate_training_prompt(conversation_text, summary),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "423018e8-b5da-40ec-8b39-2f4b4d072393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(data: Dataset):\n",
    "    return (\n",
    "        data.shuffle(seed=42)\n",
    "        .map(generate_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46e75a6-e448-442a-99cd-ba133338d969",
   "metadata": {},
   "source": [
    "## Processing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24702f7c-2623-4f40-95a3-9188f910e298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53ee7c46588a4124a36102f1b3a87b4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "165cd30e502441a6a58acde97006948f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dataset[\"train\"] = process_dataset(dataset[\"train\"])\n",
    "dataset[\"train\"] = process_dataset(dataset[\"train\"].select(range(1500)))\n",
    "dataset[\"test\"] = process_dataset(dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9970271-4854-4e51-b36c-2adf494702a0",
   "metadata": {},
   "source": [
    "## Instantiating the model and its tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76b009ae-07f4-433d-b133-e2f2a5ab733d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc8c49996c7e4a6ea0e4d81b5a5574b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447bca3d-34ce-4a24-ba00-f90b6bec26f4",
   "metadata": {},
   "source": [
    "## Quantization, Tokenizer and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c60d7f84-c3a3-4321-9c0e-a03011b331cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9448a18a-3c49-4266-b713-8afb41883a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_and_tokenizer():\n",
    "    bnb_config = BitsAndBytesConfig( #BitsAndBytes is the reponsable librarie that help us to configure the desired quantization.\n",
    "        load_in_4bit=True, # In this case, we are loading the model in a 4bir precision instead of its orginal precision.\n",
    "        bnb_4bit_quant_type=\"nf4\", # normalized float 4 bit data type\n",
    "        bnb_4bit_compute_dtype=torch.float16, # This is to use half the memory and fit the model\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained( \n",
    "        \"google/gemma-2b\", # Here we are defining the llama 2 7B model\n",
    "        use_safetensors=True, #  for storing and loading tensors\n",
    "        quantization_config=bnb_config, # with the desired quantization \n",
    "        trust_remote_code=True, \n",
    "        device_map=\"auto\" # and put each layer of the model depending on the available resources\n",
    "        \n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\") # Here we tell we want to use the same tokenizer that the model ises\n",
    "    tokenizer.pad_token = tokenizer.eos_token # Telling that all the padding tokens should be the same as the 'end of sentence'\n",
    "    tokenizer.padding_side = \"right\" # and the side padding is the right side\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179e5cb8-218d-4201-9526-dd3d2cf3165c",
   "metadata": {},
   "source": [
    "Great! Let's instatiate both model and tokenizer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8112d10-8283-488c-9733-08c618604dd1",
   "metadata": {},
   "source": [
    "Note: this may take a few minutes to run :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbdb2acf-2142-4f08-bffb-5baff2d31ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b7165d85f43473d8f2e94327f6d96f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57acb8cbce314b78af727040b50f3854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34f12c5a773c4fee8c13da3452ff626e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd51b1e761ec4202a3319ba168560f88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04a60e0a6f7a4c4596a5d112c6172094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1d06184e88747bb806dbe5fb36d3ebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ece4801c60eb4f88892fdda56670dae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db142219abb541ecaed674e5527bcabc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.11k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b49c7d601f5c435fa80614945b8765cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ea2cbd654524095915e09928391edd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a672701de2b44b50abef27919d5d33be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/555 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = create_model_and_tokenizer()\n",
    "model.config.use_cache = False # The cache is only used for generation, not for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0f7d6a-80cd-46bf-825f-af33f9b47bc5",
   "metadata": {},
   "source": [
    "## LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a219bb-1e4a-4dbb-922a-ff70be26b68d",
   "metadata": {},
   "source": [
    "If you are not familiar with LoRA, here's a brief explanation from Hugging Face:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7148e5d-b251-471d-bdd3-c2364ce8d081",
   "metadata": {},
   "source": [
    "\"To make fine-tuning more efficient, LoRA’s approach is to represent the weight updates with two smaller matrices (called update matrices) through low-rank decomposition. These new matrices can be trained to adapt to the new data while keeping the overall number of changes low. The original weight matrix remains frozen and doesn’t receive any further adjustments. To produce the final results, both the original and the adapted weights are combined.\"\n",
    "\n",
    "Source: https://huggingface.co/docs/peft/conceptual_guides/lora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ad941c-85f3-45ab-b35a-b50852bca935",
   "metadata": {},
   "source": [
    "Basically, LoRA makes fine-tuning more efficient by drastically reducing the number of trainable parameters and that's why this approach is so commum when fine tuning LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ca5564b-b988-4bee-9fa9-f60c290653ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_r = 16 # Ranking of the matrix\n",
    "lora_alpha = 64 # Scaling facotr\n",
    "lora_dropout = 0.1\n",
    "lora_target_modules = [ # Selecting what layers of the model we want to use\n",
    "    \"q_proj\",\n",
    "    \"up_proj\",\n",
    "    \"o_proj\",\n",
    "    \"k_proj\",\n",
    "    \"down_proj\",\n",
    "    \"gate_proj\",\n",
    "    \"v_proj\",\n",
    "]\n",
    "\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    target_modules=lora_target_modules,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be04388-b828-484b-8efe-a135bd65b02e",
   "metadata": {},
   "source": [
    "More details about LoRA config in this amzing blog post: https://medium.com/@manyi.yim/more-about-loraconfig-from-peft-581cf54643db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecfc897-9bb9-4958-9c87-67996526222f",
   "metadata": {},
   "source": [
    "## Training (Let's rock!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809b9c3d-a6f9-42a9-8f94-e0dedafaf0eb",
   "metadata": {},
   "source": [
    "### MLFlow setup \n",
    "We will be using MLFlow for tracking our training metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8db550-4cac-4d5d-ab30-18753ae07ebd",
   "metadata": {},
   "source": [
    "Let's define a nme for our experiment so we can easily access it on the Monitor tab later :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8331771a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MLFLOW_EXPERIMENT_NAME'] = 'gemma2b-summary_task-quant4bit-2-trial'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310291e4-4578-47ca-a5de-cd5f9162e860",
   "metadata": {},
   "source": [
    "### Training setup\n",
    "Here, we will define what are the values for epochs, optmizers, learning rate, evaluation, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814dbdad-7937-4943-9973-60024be3bac8",
   "metadata": {},
   "source": [
    "For more details, you can check Trainer docs: https://huggingface.co/transformers/v3.0.2/main_classes/trainer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed1afc98-171f-449a-8d20-a53b1e0b4114",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    per_device_train_batch_size=4, # The batch size per GPU/TPU core/CPU for training.\n",
    "    gradient_accumulation_steps=4, # Number of updates steps to accumulate the gradients for, before performing a backward/update pass.\n",
    "    optim=\"paged_adamw_32bit\", #Adam Optimizer\n",
    "    logging_steps=20, # Number of update steps between two logs if logging_strategy=\"steps\".\n",
    "    save_steps = 20,\n",
    "    learning_rate=1e-3, # The initial learning rate for AdamW optimizer\n",
    "    fp16=True, # Whether to use fp16 16-bit (mixed) precision training instead of 32-bit training.\n",
    "    max_grad_norm=0.3, # Maximum gradient norm\n",
    "    num_train_epochs=3, # Number of epochs\n",
    "    evaluation_strategy=\"steps\", # Evaluation is done (and logged) every eval_steps\n",
    "    eval_steps=0.2,\n",
    "    warmup_ratio=0.05, # Ratio of total training steps used for a linear warmup from 0 to learning_rate\n",
    "    save_strategy=\"steps\", # Save is done at the end of each steps\n",
    "    group_by_length=True, # Whether or not to group together samples of roughly the same length in the training dataset \n",
    "    output_dir='./gemma/gemma-4bit-2', # Path to save model checkpoints and bins\n",
    "    report_to=\"mlflow\", # Integration to report the results and logs to\n",
    "    save_safetensors=True, # Use safetensors saving and loading for state dicts instead of default torch.load and torch.save\n",
    "    lr_scheduler_type=\"cosine\", # The scheduler type to use.\n",
    "    seed=42, # Random seed that will be set at the beginning of training. \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "03395e7b-5470-426e-a39c-544ceb7fea24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "393273071fed46a19288b6073ddbdd13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41abaa6a71d9455997728b98ac4607d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"], # Placing our dataset on train\n",
    "    eval_dataset=dataset[\"test\"], # and test part\n",
    "    peft_config=peft_config, #LoRA\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=4096, # Specifies the maximum number of tokens of the input\n",
    "    tokenizer=tokenizer, # Model's tokenizer we loaded before\n",
    "    args=training_arguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c823cd4-08d3-4999-b645-3367ee5b7764",
   "metadata": {},
   "source": [
    "#### Star of training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb694102-85d8-48cb-9067-20fa9c34d0e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/03/06 20:07:48 INFO mlflow.tracking.fluent: Experiment with name 'gemma2b-summary_task-quant4bit-2-trial' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='279' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 50/279 52:38 < 4:11:08, 0.02 it/s, Epoch 0.52/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "trainer.train()\n",
    "end_time = time.time()\n",
    "training_duration = end_time - start_time\n",
    "mlflow.log_metric(\"training_time\", training_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d36e97f-2d22-4f54-95ef-6ae7f3b564f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e212dd2d-9220-4ac7-a989-04e713a9770e",
   "metadata": {},
   "source": [
    "## Inference time 😎"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650b1f3a-077a-4fbe-b60d-fa62a02d5b06",
   "metadata": {},
   "source": [
    "Let's load our model using PeftModel from_pretrained method like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f8f6cea6-4584-45c6-8b0c-71336f83cdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "db99c102-d39e-4aa5-9a5a-4d461db89dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(model, './gemma/gemma-4bit-2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107f7bdd-37ee-4e43-83be-f8a7e66db42b",
   "metadata": {},
   "source": [
    "Let's create our 5-rows test dataset so we can easily try the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0f5ba5f0-9ddd-46e3-9d5c-4085980dbc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(\n",
    "    conversation: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT\n",
    ") -> str:\n",
    "    return f\"\"\"### Instruction: {system_prompt}\n",
    "\n",
    "### Input:\n",
    "{conversation.strip()}\n",
    "\n",
    "### Response:\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "440ebccf-ff95-47a0-8de1-870483f59063",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = []\n",
    "\n",
    "for data_point in dataset[\"test\"].select(range(5)):\n",
    "    summary = data_point['summary']\n",
    "    conversation = data_point['dialogue']\n",
    "    examples.append(\n",
    "        {\n",
    "            \"summary\": summary,\n",
    "            \"conversation\": conversation,\n",
    "            \"prompt\": generate_prompt(conversation),\n",
    "        }\n",
    "    )\n",
    "test_df = pd.DataFrame(examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74877783-0cba-4903-860b-778837ac156c",
   "metadata": {},
   "source": [
    "And also a function to help us to tokenize the inputs and use the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "23d9ce30-7196-4567-91bd-dde2ea1355d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>conversation</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Both Claire and Linda are making curry for din...</td>\n",
       "      <td>Claire: &lt;file_photo&gt;\\r\\nKim: Looks delicious.....</td>\n",
       "      <td>### Instruction: Below is a conversation betwe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Derek and Alyssa make fun of Fergie's performa...</td>\n",
       "      <td>Alyssa: Have you seen Fergie’s national anthem...</td>\n",
       "      <td>### Instruction: Below is a conversation betwe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ann wants to buy Josh's laptop for $200. Josh ...</td>\n",
       "      <td>Ann: Hi, is the laptop still available?\\r\\nJos...</td>\n",
       "      <td>### Instruction: Below is a conversation betwe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Matt and Tony want to go to the concert of Bon...</td>\n",
       "      <td>Matt: have you heard that Bon Jovi are coming ...</td>\n",
       "      <td>### Instruction: Below is a conversation betwe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anastasia sent her new school photos to Darrell.</td>\n",
       "      <td>Anastasia: Our new school photos\\r\\nAnastasia:...</td>\n",
       "      <td>### Instruction: Below is a conversation betwe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             summary  \\\n",
       "0  Both Claire and Linda are making curry for din...   \n",
       "1  Derek and Alyssa make fun of Fergie's performa...   \n",
       "2  Ann wants to buy Josh's laptop for $200. Josh ...   \n",
       "3  Matt and Tony want to go to the concert of Bon...   \n",
       "4   Anastasia sent her new school photos to Darrell.   \n",
       "\n",
       "                                        conversation  \\\n",
       "0  Claire: <file_photo>\\r\\nKim: Looks delicious.....   \n",
       "1  Alyssa: Have you seen Fergie’s national anthem...   \n",
       "2  Ann: Hi, is the laptop still available?\\r\\nJos...   \n",
       "3  Matt: have you heard that Bon Jovi are coming ...   \n",
       "4  Anastasia: Our new school photos\\r\\nAnastasia:...   \n",
       "\n",
       "                                              prompt  \n",
       "0  ### Instruction: Below is a conversation betwe...  \n",
       "1  ### Instruction: Below is a conversation betwe...  \n",
       "2  ### Instruction: Below is a conversation betwe...  \n",
       "3  ### Instruction: Below is a conversation betwe...  \n",
       "4  ### Instruction: Below is a conversation betwe...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b4cc4188-1bec-4397-bf4b-9fc44c2be40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(model, text: str):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(DEVICE)\n",
    "    inputs_length = len(inputs[\"input_ids\"][0])\n",
    "    with torch.inference_mode():\n",
    "        # Adjust temperature to a more stable value\n",
    "        outputs = model.generate(**inputs, max_new_tokens=256, temperature=0.7)\n",
    "    return tokenizer.decode(outputs[0][inputs_length:], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0501d69-d737-4eea-ae8a-af9faeac01f6",
   "metadata": {},
   "source": [
    "### Inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9c0e2810-67d5-4744-8c67-c5d7b25d3b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "25f06264-c455-4ada-b7b9-d15b3d4ed303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claire: <file_photo>\n",
      "Kim: Looks delicious...\n",
      "Linda: No way... Look what I'm cooking right now:\n",
      "Linda: <file_photo>\n",
      "Claire: hahahaha \n",
      "Kim: Curry dream team\n",
      "Claire: Enjoy your dinner :*\n"
     ]
    }
   ],
   "source": [
    "example = test_df.iloc[0]\n",
    "print(example.conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7a1e51cc-ca7f-484c-85f4-373ea9987525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both Claire and Linda are making curry for dinner. \n"
     ]
    }
   ],
   "source": [
    "print(example.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f1ac26b1-d549-4696-a02e-9d82d6ceab52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction: Below is a conversation between friends in a chat. Write a summary of their conversation.\n",
      "\n",
      "### Input:\n",
      "Claire: <file_photo>\n",
      "Kim: Looks delicious...\n",
      "Linda: No way... Look what I'm cooking right now:\n",
      "Linda: <file_photo>\n",
      "Claire: hahahaha \n",
      "Kim: Curry dream team\n",
      "Claire: Enjoy your dinner :*\n",
      "\n",
      "### Response:\n"
     ]
    }
   ],
   "source": [
    "print(example.prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "28b5da52-dbae-4816-bab6-2bcb576a5afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32.1 s, sys: 3.12 s, total: 35.2 s\n",
      "Wall time: 35.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "summary = summarize(model, example.prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "853c53c3-4267-4dbc-9c2d-e74dad92bbfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Claire sends Kim and Linda a photo of her cooking. Linda sends Claire a photo of her cooking. Kim enjoys Claire's dinner. Claire and Kim are cooking together. Claire and Kim are curry dream team. Linda enjoys Linda's dinner. Linda is cooking. Linda sends Claire a photo of her cooking. Claire and Linda are cooking together. Linda and Claire are curry dream team. Kim enjoys Claire's dinner. Kim is cooking. Kim sends Claire a photo of her cooking. Claire and Kim are cooking together. Claire and Kim are curry dream team. Linda enjoys Linda's dinner. Linda is cooking. Linda sends Claire a photo of her cooking. Claire and Linda are cooking together. Claire and Linda are curry dream team. Kim enjoys Claire's dinner. Kim is cooking. Kim sends Claire a photo of her cooking. Claire and Kim are cooking together. Claire and Kim are curry dream team. Linda enjoys Linda's dinner. Linda is cooking. Linda sends Claire a photo of her cooking. Claire and Linda are cooking together. Claire and Linda are curry dream team. Kim enjoys Claire's dinner. Kim is cooking. Kim sends Claire a photo of her cooking. Claire and Kim are cooking together. Claire and Kim are curry dream team. Linda enjoys Linda\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42131a0a-5eec-4a57-b44f-fe398965ed24",
   "metadata": {},
   "source": [
    "You can choose other indexes as well and also try with other seeds!\n",
    "\n",
    "The major part of the outputs are not going to be very good, but sometimes there are some good outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9467cb7c-aaa3-4e52-bc8c-eef7edfc46c0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Inference using HuggignFace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee98dcdc-ea46-4567-8363-87fcae71a13c",
   "metadata": {},
   "source": [
    "The model is also available on the HuggingFace library :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8003d7e4-5c6f-4bbc-a270-b46c86ec5dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75e6d8b967b1429395486bec9bdfd8e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/688 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8964d2e439c0407d92cb0799ebe697b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee2f9f872fba4e959f1bd9f86ffded89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/78.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "config = PeftConfig.from_pretrained(\"morgana-rodrigues/gemma-2b-4bit\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\")\n",
    "model = PeftModel.from_pretrained(model, \"morgana-rodrigues/gemma-2b-4bit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "620f7b2d-877c-4fb2-8154-8066d4cc1b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do modelo quantizado de 8 bits: 78480800 bytes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_size(start_path):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(start_path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            # skip if it is symbolic link\n",
    "            if not os.path.islink(fp):\n",
    "                total_size += os.path.getsize(fp)\n",
    "\n",
    "    return total_size\n",
    "\n",
    "# Caminhos dos diretórios do modelo\n",
    "model_4bit_dir = '/home/jovyan/.cache/huggingface/hub/models--morgana-rodrigues--gemma-2b-4bit'\n",
    "\n",
    "# Obtém o tamanho do diretório do modelo quantizado de 8 bits\n",
    "size_quant_8bit = get_size(model_8bit_dir)\n",
    "\n",
    "print(f\"Tamanho do modelo quantizado de 8 bits: {size_quant_8bit} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7a78b77d-67e4-43d1-8ef8-54426a0d3dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The variable is using 125 bytes of memory.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# Example variable\n",
    "my_list = model_4bit_dir\n",
    "# Checking the size of the variable\n",
    "size_in_bytes = sys.getsizeof(my_list)\n",
    "\n",
    "print(f\"The variable is using {size_in_bytes} bytes of memory.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
