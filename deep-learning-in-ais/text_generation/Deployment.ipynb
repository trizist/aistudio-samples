{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b3f7a87-1faf-4f44-8b89-b7be46ffcc2b",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c12dca25-6a47-4674-aa23-8dd9e7afbdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "from mlflow.types.schema import Schema, ColSpec\n",
    "from mlflow.types import ParamSchema, ParamSpec\n",
    "from mlflow.models import ModelSignature\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89849458-6614-41f7-916c-ca38aec363eb",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4e89ba8-1e88-450e-bea4-4a0a99793bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('shakespeare.txt','r',encoding='utf8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "all_characters = set(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5e8c88-c69f-416a-9ac0-04f049988fc2",
   "metadata": {},
   "source": [
    "### Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "146ac375-2b3d-431c-a186-d1372e106cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, decoder, encoder, all_chars, num_hidden=256, num_layers=4,drop_prob=0.5, use_gpu=False):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.num_layers = num_layers\n",
    "        self.num_hidden = num_hidden\n",
    "        self.use_gpu = use_gpu\n",
    "        \n",
    "        self.all_chars = all_chars\n",
    "        self.decoder = torch.load(decoder)\n",
    "        self.encoder = torch.load(encoder)\n",
    "        \n",
    "        self.lstm = nn.LSTM(len(self.all_chars), num_hidden, num_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc_linear = nn.Linear(num_hidden, len(self.all_chars))\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        lstm_output, hidden = self.lstm(x, hidden)       \n",
    "        drop_output = self.dropout(lstm_output)\n",
    "        drop_output = drop_output.contiguous().view(-1, self.num_hidden)\n",
    "        final_out = self.fc_linear(drop_output)\n",
    "        \n",
    "        return final_out, hidden\n",
    "    \n",
    "    \n",
    "    def hidden_state(self, batch_size):\n",
    "        if self.use_gpu:\n",
    "            hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda(),\n",
    "                     torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda())\n",
    "        else:\n",
    "            hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden),\n",
    "                     torch.zeros(self.num_layers,batch_size,self.num_hidden))\n",
    "        \n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df33904-7221-4be1-846d-6f7a24c58cb9",
   "metadata": {},
   "source": [
    "# MLFlow - Register Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c2d2faa-e9d5-45b3-a862-8c6136bbe625",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(mlflow.pyfunc.PythonModel):\n",
    "    \n",
    "    def load_context(self, context):\n",
    "        self.model = CharModel(\n",
    "                        all_chars=all_characters,\n",
    "                        num_hidden=512,\n",
    "                        num_layers=3,\n",
    "                        drop_prob=0.5,\n",
    "                        use_gpu=False,\n",
    "                        decoder=context.artifacts['decoder'],\n",
    "                        encoder=context.artifacts['encoder']\n",
    "                                           \n",
    "                    )\n",
    "\n",
    "\n",
    "        self.model.load_state_dict(torch.load(context.artifacts['model_state_dict']))\n",
    "        self.model.eval()\n",
    "\n",
    "    def one_hot_encoder(self, encoded_text, num_uni_chars):\n",
    "        one_hot = np.zeros((encoded_text.size, num_uni_chars))\n",
    "        one_hot = one_hot.astype(np.float32)\n",
    "        one_hot[np.arange(one_hot.shape[0]), encoded_text.flatten()] = 1.0\n",
    "        one_hot = one_hot.reshape((*encoded_text.shape, num_uni_chars))\n",
    "        \n",
    "        return one_hot\n",
    "\n",
    "    def predict_next_char(self, char, hidden=None, k=3):\n",
    "        encoded_text = self.model.encoder[char]\n",
    "        encoded_text = np.array([[encoded_text]])\n",
    "        encoded_text = self.one_hot_encoder(encoded_text, len(self.model.all_chars))\n",
    "        inputs = torch.from_numpy(encoded_text)\n",
    "        inputs = inputs.cpu()\n",
    "            \n",
    "        hidden = tuple([state.data for state in hidden])\n",
    "        lstm_out, hidden = self.model(inputs, hidden)    \n",
    "        probs = F.softmax(lstm_out, dim=1).data\n",
    "        probs = probs.cpu()\n",
    "\n",
    "        \n",
    "        probs, index_positions = probs.topk(k)        \n",
    "        index_positions = index_positions.numpy().squeeze()\n",
    "        probs = probs.numpy().flatten()\n",
    "        probs = probs/probs.sum()\n",
    "        char = np.random.choice(index_positions, p=probs)\n",
    "    \n",
    "        return self.model.decoder[char], hidden\n",
    "\n",
    "    def generate_text(self, seed, size, k=3):\n",
    "\n",
    "        self.model.cpu()\n",
    "            \n",
    "        self.model.eval()\n",
    "        output_chars = [c for c in seed]\n",
    "        hidden = self.model.hidden_state(1)\n",
    "        \n",
    "        for char in seed:\n",
    "            char, hidden = self.predict_next_char(char, hidden, k=k)\n",
    "    \n",
    "        output_chars.append(char)\n",
    "        for i in range(size):\n",
    "            char, hidden = self.predict_next_char(output_chars[-1], hidden, k=k)\n",
    "            output_chars.append(char)\n",
    "            \n",
    "        return ''.join(output_chars)\n",
    "            \n",
    "        \n",
    "    def predict(self, context, model_input):\n",
    "        initial_word = model_input['initial_word'][0]\n",
    "        size = model_input['size'][0]\n",
    "        output = self.generate_text(seed=initial_word, size=size)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    @classmethod\n",
    "    def log_model(cls, model_state_dict, decoder, encoder, demo_folder=\"demo\"): \n",
    "        input_schema = Schema(\n",
    "            [\n",
    "                ColSpec(\"string\", \"initial_word\"),\n",
    "                ColSpec(\"long\", \"size\")\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        output_schema = Schema(\n",
    "            [\n",
    "                ColSpec(\"string\", \"generated_text\")\n",
    "            ]\n",
    "        )\n",
    "      \n",
    "        signature = ModelSignature(inputs=input_schema, outputs=output_schema)\n",
    "             \n",
    "        requirements = [\n",
    "            \"torch\",\n",
    "            \"numpy\"\n",
    "        ]\n",
    "        mlflow.pyfunc.log_model(\n",
    "            model_state_dict,\n",
    "            python_model=cls(),\n",
    "            artifacts={\"model_state_dict\": model_state_dict, 'decoder': decoder, 'encoder': encoder, \"demo\": demo_folder},\n",
    "            signature=signature,\n",
    "            pip_requirements=requirements\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88808f17-797b-4273-86a8-ec4be7b67bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/phoenix/mlflow/161459198344494027', creation_time=1733920341445, experiment_id='161459198344494027', last_update_time=1733920341445, lifecycle_stage='active', name='Shakespeare Text Generation', tags={}>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment(experiment_name='Shakespeare Text Generation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0e28b95-5dd5-4f36-adb3-258a85edc5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_dict = 'models/dict_torch_rnn_model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbf659ed-12e3-4941-b46f-d8cacfdfa5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "register_name = 'Shakespeare_Model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "344da6ed-dd4a-4063-86ac-f6f0168ffb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run's Artifact URI: /phoenix/mlflow/161459198344494027/58a4811f783146c5a62e9e8d96bb78d0/artifacts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6906ebb873764fe68e574c4afcd45028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e92c99de069f451e9e6f4805316a9ff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20e3b68910d247ad82c8d2b66c49d757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ee4184de3b047e1b1d015e6eb76e7dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'Shakespeare_Model' already exists. Creating a new version of this model...\n",
      "Created version '3' of model 'Shakespeare_Model'.\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name='Shakespeare_main') as run:\n",
    "    print(f\"Run's Artifact URI: {run.info.artifact_uri}\")\n",
    "    RNNModel.log_model(model_state_dict, 'models/decoder.pt', 'models/encoder.pt')\n",
    "    mlflow.register_model(model_uri = f\"runs:/{run.info.run_id}/{model_state_dict}\", name=register_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bc4e7fa-fda9-422d-bff6-b077b6c8d3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_235/863823821.py:2: FutureWarning: ``mlflow.tracking.client.MlflowClient.get_latest_versions`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n",
      "  model_metadata = client.get_latest_versions(register_name, stages=[\"None\"])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = mlflow.MlflowClient()\n",
    "model_metadata = client.get_latest_versions(register_name, stages=[\"None\"])\n",
    "latest_model_version = model_metadata[0].version\n",
    "latest_model_version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923cf27e-2a84-469c-8fe7-ab2508a4a4d4",
   "metadata": {},
   "source": [
    "#### Testing registered model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2caeb0bd-e530-461f-a30a-1a918f85a63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.state_dict of CharModel(\n",
      "  (lstm): LSTM(84, 512, num_layers=3, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc_linear): Linear(in_features=512, out_features=84, bias=True)\n",
      ")>\n",
      "Love to him; and\n",
      "    I have seen him this, though I have sent my heart\n",
      "    And we that bring the willing f\n"
     ]
    }
   ],
   "source": [
    "model = mlflow.pyfunc.load_model(model_uri=f\"models:/{register_name}/{latest_model_version}\")\n",
    "print(model.predict({\"initial_word\": 'Love ', \"size\": 100}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
