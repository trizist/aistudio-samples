{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation with Neural Networks\n",
    "In this notebook our objective is to demonstrate how to generate text using a character-based RNN working with a dataset of Shakespeare's  writing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "- Imports Dependencies\n",
    "- Define Constants and Paths and Configure Logging\n",
    "- Get Text Data\n",
    "- Preparing textual data\n",
    "- One Hot Encoding\n",
    "- Creating Training Batches\n",
    "- Creating the LSTM Model\n",
    "- Training the Network\n",
    "- Generating Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import logging\n",
    "\n",
    "# Third-Party Libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# MLflow for Experiment Tracking and Model Management\n",
    "import mlflow\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Constants and Paths and Configure Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global experiment and run names to be used throughout the notebook\n",
    "EXPERIMENT_SET = \"RNN text generation\"\n",
    "RUN_NAME = \"RNN Text Generation\"\n",
    "MODEL_NAME = \"dict_torch_rnn_model.pt\"\n",
    "\n",
    "# Set up the paths\n",
    "DATA_PATH = \"shakespeare.txt\"\n",
    "MODEL_DECODER_PATH = \"models/decoder.pt\"\n",
    "MODEL_ENCODER_PATH = \"models/encoder.pt\"\n",
    "\n",
    "# Set up the chunk separator for text processing\n",
    "CHUNK_SEPARATOR = \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the logging module with desired format and level\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "# Create a logger for this notebook\n",
    "logger = logging.getLogger('text-generation-Torch-notebook')\n",
    "logger.info(\"Logging configured successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the text we'll use as a basis for our generations: let's try to generate 'Shakespearean' texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This text is from Shakespeare's Sonnet 1. It's one of the 154 sonnets written by William Shakespeare that were first published in 1609. This particular sonnet, like many others, discusses themes of beauty, procreation, and the transient nature of life, urging the beautiful to reproduce so their beauty can live on through their offspring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_PATH,'r',encoding='utf8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('First 600 chars: \\n')\n",
    "logger.info(text[:600])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing textual data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to encode our data to give the model a proper numerical representation of our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_characters = set(text) # creates a set of unique characters found in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = dict(enumerate(all_characters))\n",
    "# assigns a unique integer to each character in a dictionary format, \n",
    "# creating a mapping that can later be used to transform encoded predictions back into characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = {char: ind for ind, char in decoder.items()} \n",
    "# reverses the decoder dictionary, providing a mapping from characters to their respective assigned integers, which is used to encode the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(decoder, MODEL_DECODER_PATH)\n",
    "torch.save(encoder, MODEL_ENCODER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = np.array([encoder[char] for char in text])\n",
    "# encodes the entire text as an array of integers, with each integer representing the character at that position\n",
    "#in the text according to the encoder dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding is a way to convert categorical data into a fixed-size vector of numerical values.\n",
    "\n",
    "This encoding allows the model to treat input data uniformly and is particularly important for models that need to determine the presence or absence of a feature, such as a particular character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(encoded_text, num_uni_chars):\n",
    "    \"\"\"\n",
    "        Convert categorical data into a fixed-size vector of numerical values.\n",
    "\n",
    "        Args:\n",
    "            encoded_text: Batch of encoded text.\n",
    "            num_uni_chars: Number of unique characters\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a placeholder for zeros\n",
    "        one_hot = np.zeros((encoded_text.size, num_uni_chars))\n",
    "        \n",
    "        # Convert data type for later use with pytorch\n",
    "        one_hot = one_hot.astype(np.float32)\n",
    "\n",
    "        # Using indexing fill in the 1s at the correct index locations\n",
    "        one_hot[np.arange(one_hot.shape[0]), encoded_text.flatten()] = 1.0\n",
    "        \n",
    "        # Reshape it so it matches the batch shape\n",
    "        one_hot = one_hot.reshape((*encoded_text.shape, num_uni_chars))\n",
    "        \n",
    "        return one_hot\n",
    "    except Exception as e:\n",
    "            logger.error(f\"Error converting categorical data: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Training Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training batches are a way of dividing the dataset into smaller, manageable groups of data points that are fed into a machine learning model during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(encoded_text, samp_per_batch=10, seq_len=50):\n",
    "    \n",
    "    '''\n",
    "    Generate (using yield) batches for training.\n",
    "    \n",
    "    X: Encoded Text of length seq_len\n",
    "    Y: Encoded Text shifted by one\n",
    "    \n",
    "    Example:\n",
    "    \n",
    "    X:\n",
    "    \n",
    "    [[1 2 3]]\n",
    "    \n",
    "    Y:\n",
    "    \n",
    "    [[ 2 3 4]]\n",
    "    \n",
    "    encoded_text : Complete Encoded Text to make batches from\n",
    "    batch_size : Number of samples per batch\n",
    "    seq_len : Length of character sequence\n",
    "       \n",
    "    '''\n",
    "    try:\n",
    "        # Total number of characters per batch\n",
    "        # Example: If samp_per_batch is 2 and seq_len is 50, then 100\n",
    "        # characters come out per batch.\n",
    "        char_per_batch = samp_per_batch * seq_len\n",
    "        \n",
    "        # Number of batches available to make\n",
    "        # Use int() to roun to nearest integer\n",
    "        num_batches_avail = int(len(encoded_text)/char_per_batch)\n",
    "        \n",
    "        # Cut off end of encoded_text that\n",
    "        # won't fit evenly into a batch\n",
    "        encoded_text = encoded_text[:num_batches_avail * char_per_batch]\n",
    "        \n",
    "        # Reshape text into rows the size of a batch\n",
    "        encoded_text = encoded_text.reshape((samp_per_batch, -1))\n",
    "\n",
    "        # Go through each row in array.\n",
    "        for n in range(0, encoded_text.shape[1], seq_len):\n",
    "            # Grab feature characters\n",
    "            x = encoded_text[:, n:n+seq_len]\n",
    "            # y is the target shifted over by 1\n",
    "            y = np.zeros_like(x)\n",
    "            try:\n",
    "                y[:, :-1] = x[:, 1:]\n",
    "                y[:, -1]  = encoded_text[:, n+seq_len]\n",
    "            except:\n",
    "                y[:, :-1] = x[:, 1:]\n",
    "                y[:, -1] = encoded_text[:, 0]\n",
    "                \n",
    "            yield x, y\n",
    "        logger.info(\"Batches generated successfully\")\n",
    "    except Exception as e:\n",
    "            logger.error(f\"Error Generating batches: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharModel(nn.Module):\n",
    "    def __init__(self, all_chars, num_hidden=256, num_layers=4,drop_prob=0.5, use_gpu=False):\n",
    "        \"\"\"Initializes CharModel\n",
    "\n",
    "        Args:\n",
    "            all_chars: Set of unique characters found in the text.\n",
    "            num_hidden: Number of hidden layers. Defaults to 256.\n",
    "            num_layers: Number of layers. Defaults to 4.\n",
    "            drop_prob: Regularization technique to prevent overfitting. Defaults to 0.5.\n",
    "            use_gpu: If the model uses GPU. Defaults to False.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            super().__init__()\n",
    "            self.drop_prob = drop_prob\n",
    "            self.num_layers = num_layers\n",
    "            self.num_hidden = num_hidden\n",
    "            self.use_gpu = use_gpu\n",
    "            \n",
    "            self.all_chars = all_chars\n",
    "            self.decoder = torch.load(MODEL_DECODER_PATH)\n",
    "            self.encoder = torch.load(MODEL_ENCODER_PATH)\n",
    "            \n",
    "            self.lstm = nn.LSTM(len(self.all_chars), num_hidden, num_layers, dropout=drop_prob, batch_first=True)\n",
    "            self.dropout = nn.Dropout(drop_prob)\n",
    "            self.fc_linear = nn.Linear(num_hidden, len(self.all_chars))\n",
    "            logger.info(\"CharModel Initialized successfully\")\n",
    "    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error Initializing CharModel: {str(e)}\")\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"Implementation of the CharModel logic, in which, the input passes through every step of the arquiteture\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor with shape (batch size and senquency length) containing character indices.\n",
    "            hidden: Tuple containing the inicial hidden states of the CharModel each with shape (batch size and senquency length).\n",
    "\n",
    "        Returns:\n",
    "            final_out: Output tensor representing the predicted logits for each character in the sequence.\n",
    "            hidden: Tuple containing the final hidden states of the CharModel.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            lstm_output, hidden = self.lstm(x, hidden)       \n",
    "            drop_output = self.dropout(lstm_output)\n",
    "            drop_output = drop_output.contiguous().view(-1, self.num_hidden)\n",
    "            final_out = self.fc_linear(drop_output)\n",
    "            \n",
    "            return final_out, hidden\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error implementing CharModel logic: {str(e)}\")\n",
    "    \n",
    "    \n",
    "    def hidden_state(self, batch_size):\n",
    "        \"\"\"\n",
    "        Initializes and returns the initial hidden state for a recurrent neural network (e.g., LSTM).\n",
    "\n",
    "        This method creates zero-filled tensors for the hidden state (h_0) and cell state (c_0), \n",
    "        supporting GPU execution if `self.use_gpu` is set to True.\n",
    "\n",
    "        Args:\n",
    "            batch_size: The number of sequences in the input batch, used to determine the tensor dimensions.\n",
    "\n",
    "        Returns:\n",
    "            Tuple: A tuple containing the hidden state and cell state tensors \n",
    "            with shape (num_layers, batch_size, num_hidden). Returns None if an exception occurs, and logs the error.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self.use_gpu:\n",
    "                hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda(),\n",
    "                        torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda())\n",
    "            else:\n",
    "                hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden),\n",
    "                        torch.zeros(self.num_layers,batch_size,self.num_hidden))\n",
    "            \n",
    "            return hidden\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error Initializing and returning the initial hidden state: {str(e)}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instance of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharModel(\n",
    "    all_chars=all_characters,\n",
    "    num_hidden=512,\n",
    "    num_layers=3,\n",
    "    drop_prob=0.5,\n",
    "    use_gpu=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of data to be used for training\n",
    "train_percent = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(len(encoded_text) * (train_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ind = int(len(encoded_text) * (train_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = encoded_text[:train_ind]\n",
    "val_data = encoded_text[train_ind:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epochs to train for\n",
    "epochs = 30\n",
    "# batch size \n",
    "batch_size = 128\n",
    "# Length of sequence\n",
    "seq_len = 100\n",
    "# for printing report purposes\n",
    "# always start at 0\n",
    "tracker = 0\n",
    "# number of characters in text\n",
    "num_char = max(encoded_text)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(EXPERIMENT_SET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.start_run(run_name = RUN_NAME)\n",
    "\n",
    "mlflow.log_param(\"epochs\", epochs)\n",
    "mlflow.log_param(\"batch_size\", batch_size)\n",
    "\n",
    "# Set model to train\n",
    "model.train()\n",
    "\n",
    "# Check to see if using GPU\n",
    "if model.use_gpu:\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "    model.cuda()\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    hidden = model.hidden_state(batch_size)\n",
    "    \n",
    "    \n",
    "    for x,y in generate_batches(train_data, batch_size, seq_len):\n",
    "        \n",
    "        tracker += 1\n",
    "        \n",
    "        # One Hot Encode incoming data\n",
    "        x = one_hot_encoder(x, num_char)\n",
    "        \n",
    "        # Convert Numpy Arrays to Tensor\n",
    "        inputs = torch.from_numpy(x)\n",
    "        targets = torch.from_numpy(y)\n",
    "        \n",
    "        # Adjust for GPU if necessary\n",
    "        if model.use_gpu:\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "            \n",
    "        # Reset Hidden State\n",
    "        hidden = tuple([state.data for state in hidden])\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        lstm_output, hidden = model.forward(inputs, hidden)\n",
    "        loss = criterion(lstm_output, targets.view(batch_size*seq_len).long())\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Clipping gradients to avoid explosion\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if tracker % 100 == 0:\n",
    "            val_hidden = model.hidden_state(batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            \n",
    "            for x,y in generate_batches(val_data, batch_size, seq_len):\n",
    "                x = one_hot_encoder(x, num_char)\n",
    "                inputs = torch.from_numpy(x)\n",
    "                targets = torch.from_numpy(y)\n",
    "                \n",
    "                if model.use_gpu:\n",
    "                    inputs = inputs.cuda()\n",
    "                    targets = targets.cuda()\n",
    "                \n",
    "                val_hidden = tuple([state.data for state in val_hidden])\n",
    "                \n",
    "                lstm_output, val_hidden = model.forward(inputs, val_hidden)\n",
    "                val_loss = criterion(lstm_output, targets.view(batch_size*seq_len).long())\n",
    "        \n",
    "                val_losses.append(val_loss.item())\n",
    "            \n",
    "  \n",
    "            mlflow.log_metric(\"Val Loss\", val_loss.item(), step=tracker)\n",
    "        \n",
    "            model.train()\n",
    "            \n",
    "    logger.info(f\"Epoch: {i} Step: {tracker} Val Loss: {val_loss.item()}\")\n",
    "\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'models/{model_name}')\n",
    "logger.info(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharModel(\n",
    "    all_chars=all_characters,\n",
    "    num_hidden=512,\n",
    "    num_layers=3,\n",
    "    drop_prob=0.5,\n",
    "    use_gpu=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(f'models/{model_name}'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_char(model, char, hidden=None, k=1):\n",
    "    \"\"\"\n",
    "    Predicts the next character given an input character and the current hidden state.\n",
    "\n",
    "    This method encodes the input character, feeds it through the trained character-level \n",
    "    language model (e.g., LSTM), and samples from the top-k most probable characters \n",
    "    to determine the next one. It also returns the updated hidden state for sequential prediction.\n",
    "\n",
    "    Args:\n",
    "        char: The input character to start prediction from.\n",
    "        hidden: Current hidden state of the model. Each tensor has shape (num_layers, batch_size, num_hidden).\n",
    "            If None, a new hidden state should be initialized before calling this method.\n",
    "        k: Number of top predictions to sample from.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the predicted next character and the updated hidden state.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        encoded_text = model.encoder[char]\n",
    "        encoded_text = np.array([[encoded_text]])\n",
    "        encoded_text = one_hot_encoder(encoded_text, len(model.all_chars))\n",
    "        inputs = torch.from_numpy(encoded_text)\n",
    "    \n",
    "        if(model.use_gpu):\n",
    "            inputs = inputs.cuda()  \n",
    "\n",
    "        hidden = tuple([state.data for state in hidden])\n",
    "        lstm_out, hidden = model(inputs, hidden)        \n",
    "        probs = F.softmax(lstm_out, dim=1).data\n",
    "    \n",
    "        if(model.use_gpu):\n",
    "            probs = probs.cpu()\n",
    "\n",
    "    # Getting the top 'k' for next char probs\n",
    "        probs, index_positions = probs.topk(k)        \n",
    "        index_positions = index_positions.numpy().squeeze()\n",
    "        probs = probs.numpy().flatten()\n",
    "        probs = probs/probs.sum()\n",
    "        char = np.random.choice(index_positions, p=probs)    \n",
    "    \n",
    "        return model.decoder[char], hidden\n",
    "    except Exception as e:\n",
    "            logger.error(f\"Error predicting next char: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, size, seed='The', k=1):\n",
    "    \"\"\"\n",
    "    Generates a sequence of text using the trained character-level language model.\n",
    "\n",
    "    Starting from a seed string, this method uses the model to predict the next character\n",
    "    one at a time, feeding each predicted character back into the model. It continues\n",
    "    this process until the desired output length is reached.\n",
    "\n",
    "    Args:\n",
    "        seed: The initial sequence of characters used to start the text generation.\n",
    "        size: The number of characters to generate after the seed.\n",
    "        k: Number of top character predictions to consider for sampling at each step.\n",
    "\n",
    "    Returns:\n",
    "        The full generated text including the seed and the newly predicted characters.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if(model.use_gpu):\n",
    "            model.cuda()\n",
    "        else:\n",
    "            model.cpu()\n",
    "            \n",
    "        model.eval()\n",
    "        output_chars = [c for c in seed]\n",
    "        hidden = model.hidden_state(1)\n",
    "        \n",
    "        for char in seed:\n",
    "            char, hidden = predict_next_char(model, char, hidden, k=k)\n",
    "\n",
    "        output_chars.append(char)\n",
    "        for i in range(size):\n",
    "            char, hidden = predict_next_char(model, output_chars[-1], hidden, k=k)\n",
    "            output_chars.append(char)\n",
    "            \n",
    "        return ''.join(output_chars)\n",
    "    except Exception as e:\n",
    "            logger.error(f\"Error making predictions: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating a text with 1000 chars starting with word 'Confidence'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text(model, 1000, seed='Confidence ', k=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating a text with 1000 chars starting with word 'Love'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text(model, 1000, seed='Love ', k=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Built with ❤️ using Z by HP AI Studio."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
