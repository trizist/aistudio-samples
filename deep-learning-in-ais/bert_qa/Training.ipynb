{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using pre-trained BERT for building a Q&A system"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **üòâ Hello there!**\n",
    "\n",
    "Before starting to explore and run this notebook, here are a few things you should know:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>üßê What is BERT?</b>\n",
    "\n",
    "It is a language representation model which stands for Bidirectional Encoder Representations from Transformers. BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications. See more details in the official paper [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.](https://arxiv.org/abs/1810.04805)\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>üßê What is transformers?</b>\n",
    "\n",
    "[Transformers](https://huggingface.co/docs/transformers/index) provides APIs and tools to easily download and train state-of-the-art pretrained models. Using pretrained models can reduce your compute costs, carbon footprint, and save you the time and resources required to train a model from scratch. These models support common tasks in different modalities, such as:\n",
    "\n",
    "- Natural Language Processing\n",
    "- Computer Vision\n",
    "- Audio\n",
    "- Multimodal\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>üßê What is HuggingFace?</b>\n",
    "\n",
    "[Hugging Face, Inc.](https://huggingface.co/) is a French company that develops tools for building applications using machine learning. It is most notable for its Transformers library built for natural language processing applications and its platform that allows users to share machine learning models and datasets. The [HuggingFace Hub ](https://huggingface.co/docs/hub/index) is a platform with over 120k models, 20k datasets, and 50k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together. The Hub works as a central place where anyone can explore, experiment, collaborate and build technology with Machine Learning.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all the libraries and frameworks we're going to use\n",
    "from datasets import load_dataset\n",
    "from evaluate import load as load_metric\n",
    "from datetime import datetime\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import torch\n",
    "from tqdm.autonotebook import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_all_execution = datetime.now() # This variable is to help us to see in how much time this notebook will run"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>üßê What is the SQuAD dataset?</b>\n",
    "\n",
    "Stanford Question Answering Dataset ([SQuAD](https://rajpurkar.github.io/SQuAD-explorer/)) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable. See more details at SQuAD.\n",
    "\n",
    "In this case, we're going to download this dataset from the [HuggingFace datasets](https://huggingface.co/datasets) repository.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_dataset = load_dataset(\"squad\") # Downloading the dataset\n",
    "squad_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By printing the squad_dataset variable, we can see it is a dict composed by two key-values: the 'train' key, with all the train dataset as value, and the 'validation' key, with all the validation dataset as value. In this first part, we're using just the train dataset. Let's explore it a little bit, shall we?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **The train dataset**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw, we have and dict with two key-values. Let's access the \"train\" key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_train_dataset = squad_dataset['train']\n",
    "squad_train_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, inside the squad_train_dataset, we have another type of data, which is a Dataset type, (very similar to a dict) composed by two key-values: 'features' and 'num_rows'. \n",
    "We have in this Dataset 87599 rows, which corresponds to 87599 indixes inside it, each index corresponds to one question-anwser input for our model, like this:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_input = 1\n",
    "\n",
    "squad_train_dataset[index_input]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking into the this input of this Dataset object, we can see our first input.\n",
    "\n",
    "The inputs are a dict where each key comes from the 'features' list we just seen earlier, so we have:\n",
    "\n",
    "- id: A unique id for each input\n",
    "- title: The title for the question-answer (to give context)\n",
    "- context: The text input for the model to search the answer for the question\n",
    "- question: The question based on the context\n",
    "- answers: The answer based on the context\n",
    "\n",
    "Each of this feature can be accessed individually, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(squad_train_dataset[index_input]['title'])\n",
    "print(squad_train_dataset[index_input]['context'])\n",
    "print(squad_train_dataset[index_input]['question'])\n",
    "print(squad_train_dataset[index_input]['answers'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something important to notice here is that the 'answers' is an key composed by a dict as a value with two key-values: 'text' and 'answer_start'.\n",
    "The 'text' key corresponds to a list with the text answers, so yes! We can have more than one answer for each question in the datasets!\n",
    "In this case, for the first input we just have one answer. But we should check in the rest of the inputs. Let's do this using the filter method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking in the train dataset if we have just one answer for each question\n",
    "squad_train_dataset.filter(lambda x: len(x['answers']['text']) != 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we don't have more than one answer for each question. \n",
    "\n",
    "Well, we're done exploring the train dataset. Let's go to the next part, shall we?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>üßê What is a tokenizer?</b>\n",
    "\n",
    "A tokenizer is in charge of preparing the inputs for a model. There are a range of tokenizers, so let's get to know the BERT tokenizer and what it does with the text.\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **BERT Tokenizer**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing we have to do is to load the [BERT](https://huggingface.co/docs/transformers/model_doc/bert) pre-trained model from the HuggigngFace Hub. In this case, we're using the [distilbert-base-cased](https://huggingface.co/distilbert-base-cased) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_bbc = \"distilbert-base-cased\" #\"bert-base-cased\" is a larger option if you want to test!\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint_bbc) # getting the model's tokenizer "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For exploring it, let's take the same sample from the train dataset exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting just context and the question\n",
    "#print(index_input) # uncoment to remember the index\n",
    "\n",
    "context = squad_train_dataset[index_input]['context'] \n",
    "question = squad_train_dataset[index_input]['question']\n",
    "print(context)\n",
    "print(question)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pass the question and the context for the tokenizer, in this order, since it is the way BERT receives the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(question, context) # Decoding the inputs\n",
    "inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that it returned a big dict. Let's chekc its keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 2 keys here, but in this experiment, we're just interested in the 'input_ids', okay? Let's understand it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **input_ids**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the first 26 items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs['input_ids'][0:27])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can be asking yourself what are those numbers. Let's decode them with the same tokenizer we've just used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(inputs['input_ids'][0:27])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like magic, right? üòå\n",
    "\n",
    "What the tokenizer did was to concatenate both the question and the context in one long string and give as output the dict with two keys, where the input_ids has a list of each token of the original context and answer in a numerical representation. For example, the token \"What\" from the text is corresponding to the input_id 1327."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, and how about the tokens [CLS] and [SEP]?\n",
    "\n",
    "Don't worry, I got you! These are called special tokens. CLS is the classification token and comes before the question. The SEP token is to delimit the begining and the end of the context. So, the format that BERT outputs is follows is:\n",
    "\n",
    "**[CLS] 'question' [SEP] 'context' [SEP]**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **BERT and (way too) long contexts**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that the context of the inputs can be very long. Let's get the context from the previous part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = 0 \n",
    "for i in context:\n",
    "    if str(i) == '.':\n",
    "        sentences += 1\n",
    "\n",
    "print(context)\n",
    "print(f\"Total number of words: {len(context)}\")\n",
    "print(f\"Total number of sentences: {sentences}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was something that would worry in the NLP area, since it's pretty different from other commom application like next sentence predictions where the inputs were just single sentences, and not just that: BERT can only handle a limited number of tokens! (In 2023, right now, it is limited to 512 tokens). You could think \"Why we just don't truncate the context?\". Well, this is a terrible option since our answer could be cut off from it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution that was found is to split the context into **multiple context windows**!\n",
    "It means that one data sample will turn into multiple data samples, and at least one of them will certanly contain the answer.\n",
    "\n",
    "But, what if a part of the answer begins in one window and is cut off and then the rest is in the next window?\n",
    "\n",
    "Well, in this case, we use **overlaping windows**!\n",
    "It is easier to understand and visualize all of those concepts when we use the tokenizer again. Let's go!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Understanding the model's tokenizer**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we're using the same quenstion and context from the previous part and also passing them to the tokenizer in this specific order. \n",
    "\n",
    "- max_length: refers to the maximum length of the entire input (including the question, context and special tokens [CLS] and [SEP]). \n",
    "- truncation: here, we're saying that we just want to truncate the second input, which is the context.\n",
    "- stride: this one defines how much overlap there is between the context windows when they're splited up.\n",
    "- return_overflowing_tokens: this one is to return the overlaping tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "    question,\n",
    "    context, \n",
    "    max_length=100,\n",
    "    truncation=\"only_second\",\n",
    "    stride=50,\n",
    "    return_overflowing_tokens=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what we've got in the inputs and its keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs)\n",
    "print(inputs.keys())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the 'attention_mask' key appears again, we're just interested in the other 2 keys. Let's dive in!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **input_ids**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see now that the 'input_ids' is now a list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total number of lists: {len(inputs[\"input_ids\"])}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's decode those lists and see what we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in inputs[\"input_ids\"]:\n",
    "    print(tokenizer.decode(id))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wonderful! The tokenizer has splited the context into 4 different inputs and has conserved the question and the special tokens in each one of them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **overflow_to_sample_mapping**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do a better demonstration of what is this new key, let's pass to the tokenizer more than one input sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_samples = squad_train_dataset[:3][\"question\"] # Getting the first 3 questions\n",
    "context_samples = squad_train_dataset[:3][\"context\"] #  and contexts\n",
    "\n",
    "for i in question_samples:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(context) # You can check if you want but those questions are from the same context, so no need to print all of the 3."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up the tokenizer with these new samples and one more argument to understand the overflow mapping:\n",
    "\n",
    "- return_offsets_mapping: this returns the start and end character for each token (it will be explained after this part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "    question_samples, \n",
    "    context_samples,\n",
    "    max_length=100,\n",
    "    truncation=\"only_second\",\n",
    "    stride=50,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many windows we have now with 3 inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(inputs[\"input_ids\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! But how do we know how much each input was splitted on? That's what the overflow_to_sample_mapping gives to us! Look at this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[\"overflow_to_sample_mapping\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first sample corresponds to 0, so it was splied into 4 windows. The second sample corresponds to 1, and it was also splitted into 4 windows. The for the third sample, which is number 2. Since they como from the same context, it's usual to have the same amount of windows for the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in inputs[\"input_ids\"]:\n",
    "    print(tokenizer.decode(id))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **offset_mapping**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do a better demonstration, let's go back to the single input and pass to the same tokenizer from the previous part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(question, \"\\n\", context) #descoment this cell if you don't remember them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "    question,\n",
    "    context, \n",
    "    max_length=100,\n",
    "    truncation=\"only_second\",\n",
    "    stride=50,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True\n",
    ")\n",
    "\n",
    "inputs.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now a new key, the offset_mapping key. Let's print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs['offset_mapping']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that it is a list composed of lists of tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of lists: {len(inputs['offset_mapping'])}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, the offset_mapping tell us the location of  the start and the end of each token from the **ORIGINAL** samples! Take a look at this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(inputs['input_ids'][0])) # Taking the firts window\n",
    "print(inputs['offset_mapping'][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the sentence and the offset_mapping, the (0,0) is for the special tokens. The (0,4) is for the location of the token \"What\", which is composed by 4 chars, so it starts at the position 0 and ends at position 4. The next is the (5, 7) for the location of the token \"is\", which starts at position 5 and ends at position 7, and so on."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the word 'original' from the previous text is in bold and upper case. Let's understand why this is important in the next step."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Targets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have just seen that we can work with long contexts by spliting them into multiple windows, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in inputs[\"input_ids\"]:\n",
    "    print(tokenizer.decode(id))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Well, the thing is that the answer in the dataset comes with a start position, remember?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = squad_train_dataset[index_input]['answers']\n",
    "print(answer)\n",
    "#print(context) # uncoment here to remember the original context"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this start position is within the original context that has not been splited yet. After spliting it into windows of context, that position is no longer valid, and for the model, this is exactly the target that it is waiting for. So, we need to align this targets within the windows now, considering that sometimes the answer may not exist in one specific window or only exist in part."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's an useful method we can call from the tokenizer from all models (e.g. DistilBERT, bert-base-uncased, etc) which is the sequence_ids method. Let's take a look."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **sequence_ids method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs.sequence_ids(0)) # Getting the first window\n",
    "print(tokenizer.decode(inputs['input_ids'][0])) # first window context"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The meaning of each item is what follows:\n",
    "\n",
    "- None: special tokens like [CLS] and [SEP]\n",
    "- 0: is par of the question sentence\n",
    "- 1: is par of the the context sentence\n",
    "\n",
    "So, with this codification, we can now compare the start position of the answer in the original context and the start position of the answer in the windows!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **Finding the answer: Window contexts and original context**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see now what is the start index of the answer within the windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_ids = inputs.sequence_ids(0) # Getting the first window\n",
    "\n",
    "wind_ctx_start = sequence_ids.index(1) # Getting the first occurence of 1, which means the index where the context begins\n",
    "wind_ctx_end = (len(sequence_ids) - sequence_ids[::-1].index(1) - 1) # Getting the index of the last 1, where the context ends\n",
    "\n",
    "wind_ctx_start, wind_ctx_end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the original start answer position (within the context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer)\n",
    "ans_start_char = answer['answer_start'][0]\n",
    "ans_end_char = ans_start_char + len(answer['text'][0]) # the length of the text plus 515 is the final char of the answer\n",
    "\n",
    "print((ans_start_char, ans_end_char))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! Let's now use the offset mapping since it tells us about the char positions within the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = inputs['offset_mapping'][0] # First windows\n",
    "# print(offset) # uncoment to remember the offset\n",
    "# print(tokenizer.decode(inputs['input_ids'][0])) # and how they correspond to the original sentences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the offset has the original char starts and ends and also the (0,0) for special tokens (in this case, we are focosing in the [SEP] token that tell us where the context starts and ends) in the original context, we can compare if those original indeces match with the window context indices we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_idx = 0\n",
    "end_idx = 0\n",
    "\n",
    "if offset[wind_ctx_start][0] > ans_start_char or offset[wind_ctx_end][1] < ans_end_char:\n",
    "    print(\"target is (0,0)\")\n",
    "else:\n",
    "    i = wind_ctx_start\n",
    "    for start_end_char in offset[wind_ctx_start:]:\n",
    "        start, end = start_end_char\n",
    "        if start == ans_start_char:\n",
    "            start_idx = i\n",
    "\n",
    "        if end == ans_end_char:\n",
    "            end_idx = i \n",
    "            break\n",
    "\n",
    "        i += 1\n",
    "    \n",
    "start_idx, end_idx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to get this indexes and use in the input_ids from the first sample and then decode it to see if the answers match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = inputs['input_ids'][0]\n",
    "# tokenizer.decode(input_ids) # uncoment to visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placing the start_idx and end_idx and decoding.\n",
    "print(input_ids[start_idx:end_idx+1])\n",
    "print(tokenizer.decode(input_ids[start_idx : end_idx + 1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer['text']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! It matches!\n",
    "\n",
    "Now, we just have to turn this into a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_asnwer_token_idx(\n",
    "        ctx_start,\n",
    "        ctx_end,\n",
    "        ans_start_char,\n",
    "        ans_end_char,\n",
    "        offset\n",
    "):\n",
    "\n",
    "    start_idx = 0\n",
    "    end_idx = 0\n",
    "\n",
    "    if offset[ctx_start][0] > ans_start_char or offset[ctx_end][1] < ans_end_char:\n",
    "        pass #answer does not exist\n",
    "    else:\n",
    "        i = ctx_start\n",
    "        # aligning the indices of the answers within the context windows\n",
    "        for start_end_char in offset[ctx_start:]:\n",
    "            start, end = start_end_char\n",
    "            if start == ans_start_char:\n",
    "                start_idx = i\n",
    "\n",
    "            if end == ans_end_char:\n",
    "                end_idx = i \n",
    "                break\n",
    "\n",
    "            i += 1\n",
    "    return start_idx, end_idx\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now applying to the whole dataset\n",
    "start_idxs = []\n",
    "end_idxs = []\n",
    "\n",
    "for i, offset in enumerate(inputs[\"offset_mapping\"]):\n",
    "    sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "    ctx_start = sequence_ids.index(1)\n",
    "    ctx_end = len(sequence_ids) - sequence_ids[::-1].index(1) - 1\n",
    "\n",
    "    start_idx, end_idx = find_asnwer_token_idx(\n",
    "        ctx_start,\n",
    "        ctx_end,\n",
    "        ans_start_char,\n",
    "        ans_end_char,\n",
    "        offset\n",
    "    )\n",
    "\n",
    "    start_idxs.append(start_idx)\n",
    "    end_idxs.append(end_idx)\n",
    "\n",
    "start_idxs, end_idxs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are in this format because of the overlapping, remember?\n",
    "In this input we have 4 windows, which means that for the firts window, the answer starts at index 53 and ends in index 57. Same for the second window. For the third and last windows, the answer does not appear. üòâ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One commum issue in this dataset is that some questions are badly formatted and have extra white spaces in the beggining of in the end of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in squad_dataset[\"train\"][\"question\"][:1000]:\n",
    "    if q.strip() != q:\n",
    "        print(q)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's define our tokenizer function and add this particular par for dealing with extra white spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some fixed args\n",
    "max_length = 384 # Indicated by Google\n",
    "stride = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn_train(batch):\n",
    "    questions = [q.strip() for q in batch['question']]\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        questions, \n",
    "        batch['context'],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # We won't use those guys so let's kick them off (remove them)\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    orig_sample_idxs = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    # From the original dataset\n",
    "    answers = batch['answers']\n",
    "    start_idxs, end_idxs = [], []\n",
    "\n",
    "    # Loops we just saw\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = orig_sample_idxs[i]\n",
    "        answer = answers[sample_idx]\n",
    "\n",
    "        ans_start_char = answer['answer_start'][0]\n",
    "        ans_end_char = ans_start_char + len(answer['text'][0])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Aligning the indexes\n",
    "        ctx_start = sequence_ids.index(1)\n",
    "        ctx_end = len(sequence_ids) - sequence_ids[::-1].index(1) - 1\n",
    "\n",
    "        start_idx, end_idx = find_asnwer_token_idx(\n",
    "            ctx_start,\n",
    "            ctx_end,\n",
    "            ans_start_char,\n",
    "            ans_end_char,\n",
    "            offset\n",
    "        )\n",
    "\n",
    "        start_idxs.append(start_idx)\n",
    "        end_idxs.append(end_idx)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_idxs\n",
    "    inputs[\"end_positions\"] = end_idxs\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = squad_train_dataset.map(\n",
    "    tokenize_fn_train,\n",
    "    batched=True,\n",
    "    remove_columns=squad_train_dataset.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The actual train dataset ir a little bit  bigger than the original \n",
    "# Because we've expanded the context in windows\n",
    "print(f'Processed dataset: {len(train_dataset)}\\nOriginal dataset: {len(squad_dataset[\"train\"])}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the same function for the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn_validation(batch):\n",
    "    questions = [q.strip() for q in batch['question']]\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        questions, \n",
    "        batch['context'],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    orig_sample_idxs = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    sample_ids = []\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        # Getting the corresponding ID from the original samples (thei identify the questions and contexts remember?) \n",
    "        sample_idx = orig_sample_idxs[i]\n",
    "        sample_ids.append(batch[\"id\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i) # 1:context | 0:question | (0,0): special tokens\n",
    "        offset = inputs[\"offset_mapping\"][i] # getting the sequence_ids for this sample\n",
    "\n",
    "        # Modifying the original offset_mapping \n",
    "        # When it is (0,0) or 0 replace with None\n",
    "        # And get just the context\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            x if sequence_ids[j] == 1 else None for j, x in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "    inputs[\"sample_id\"] = sample_ids\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = squad_dataset[\"validation\"].map(\n",
    "    tokenize_fn_validation,\n",
    "    batched=True,\n",
    "    remove_columns=squad_dataset[\"validation\"].column_names\n",
    ")\n",
    "\n",
    "print(f'Processed dataset: {len(validation_dataset)}\\nOriginal dataset: {len(squad_dataset[\"validation\"])}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Metrics and Logits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load a metric called \"squad\" anduse it in our problem! Let's see how it will work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making some examples\n",
    "\n",
    "pred_answers = [\n",
    "    {'id': '1', 'prediction_text': 'Strawberry'},\n",
    "    {'id': '2', 'prediction_text': 'Agriculture industry'},\n",
    "    {'id': '3', 'prediction_text': 'Red'}\n",
    "]\n",
    "\n",
    "true_answers = [\n",
    "    {'id': '1', 'answers': {'text': ['Strawberry'], 'answer_start': [80]}},\n",
    "    {'id': '2', 'answers': {'text': ['Agroindustry'], 'answer_start': [65]}},\n",
    "    {'id': '3', 'answers': {'text': ['Red'], 'answer_start': [100]}}\n",
    "]\n",
    "\n",
    "# checking the metrics\n",
    "\n",
    "metric.compute(predictions=pred_answers, references=true_answers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, before heading to metrics, let's remember that the model outputs are Logits, numbers! We have to make it back to numbers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For that, we're donwloading a pretrained question-answering model to get predictions that are not random and use those predictions to learn how to convert the logits into answer strings.\n",
    "\n",
    "With it, we won't need the whole dataset, but just a part of it for learning how to turn them into strings!\n",
    "\n",
    "Let's dive in!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Learning how to transform logits into answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_validation_dataset = squad_dataset[\"validation\"].select(range(100)) # Getting just the first 100 samples from the validation set \n",
    "trained_checkpoint = \"distilbert-base-cased-distilled-squad\" # model trained in q&a\n",
    "\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(trained_checkpoint) # new tokenizer from distilbert-base-cased-distilled-squad\n",
    "\n",
    "# Here, since the tokenizer is a global variable \n",
    "# And we're training it with another model trained in q&a\n",
    "# We're temporarily exchanging this global variable for the tokenizer2\n",
    "original_tokenizer = tokenizer\n",
    "tokenizer = tokenizer2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's process this small validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_validation_processed = small_validation_dataset.map( # Now, we can use this new tokenizer from distilbert-base-cased-distilled-squad\n",
    "    tokenize_fn_validation,                                 # and map it into our small validation dataset using the function tokenize_fn_validation\n",
    "    batched=True,\n",
    "    remove_columns=squad_dataset[\"validation\"].column_names\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this cell is done, let's just get back with the first and original tokenizer from distilbert-base-cased model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = original_tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's time to change some things in our small dataset like unsed columns and change to torch format in order to pass the inputs to process in the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_model_inputs =  small_validation_processed.remove_columns(['sample_id', 'offset_mapping']) # unused columns\n",
    "small_model_inputs.set_format(\"torch\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting the GPU"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the previous step is done, it's time to set the GPUas our device and move the inputs (now tensors) to there "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the GPU as current device \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_model_inputs_gpu = {\n",
    "    k: small_model_inputs[k].to(device) for k in small_model_inputs.column_names\n",
    "}\n",
    "# All the data will come from the GPU now"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading the distilbert-base-cased-distilled-squad model and setting into the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model =  AutoModelForQuestionAnswering.from_pretrained(trained_checkpoint).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the model's output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad(): # This is just saying that we're not using any compution gradient (like we're not training)\n",
    "    outputs = trained_model(**small_model_inputs_gpu) # passing the inputs to distilbert-base-cased-distilled-squad and getting the outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Let's see those outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kind of QuestionAnsweringModelOutput object is composed with a tuple containing the start_logits and the end_logits."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Turning the logits into IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we're getting the logits, moving back to CPU and formatting as a numpy array (we don't need them in the tensor format anymore)\n",
    "start_logits = outputs.start_logits.cpu().numpy()\n",
    "end_logits = outputs.end_logits.cpu().numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remember how the ID's look like in the small_validation_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_validation_processed[\"sample_id\"][:3] # remember that small_validation_processed was processed by distilbert-base-cased-distilled-squad tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And how they look like in our validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset[\"sample_id\"][:3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, they're not unique! Remebember that one ID could come from more than one question-answer input because of the windows? One input could be splitted into 2 or 3 or more windows, but they still form the same input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total ID's from validation: {len(validation_dataset['sample_id'])},\\nTotal unique ID's from validation: {len(set(validation_dataset['sample_id']))}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for handling this case, we're building an dict where the key is the ID and the value is going to be an list pointing the indixes taht this ID corresponds in the small_validation_processed. If we have an input that was splited into 3 parts for examples, we want something like this:\n",
    "\n",
    "{'56be4db0acb8001400a502ef': [5, 6, 7]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_id2idxs = {}\n",
    "\n",
    "for i, id_ in enumerate(small_validation_processed['sample_id']): # looping through all the ID's and enumerating to get the index\n",
    "    if id_ not in sample_id2idxs: # Checking if this ID existis\n",
    "        sample_id2idxs[id_] = [i] # If not, we create an entry with the format we just saw above.\n",
    "    else:\n",
    "        print(\"here\") # If existis,\n",
    "        sample_id2idxs[id_].append(i) # we just append into the existing list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_id2idxs # uncoment to see the result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now, let's understand how we're turning this into strings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the shape of our logits. We expect them to be in this shape:\n",
    "\n",
    "(number_of_samples, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_logits.shape, end_logits.shape # remember that they come from the outputs we got from distilbert-base-cased-distilled-squad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to sort the indices of the logits in order to get where the values are stored within this indices.\n",
    "First, we negate the the values by placing a '-' in front of the array. This will sort them in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncoment to see\n",
    "#print(start_logits[0])\n",
    "#print()\n",
    "#print(-start_logits[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In thsi way, the largest values will be at the front. Then, when we call the argsort method, we organize this array by ascending order, getting this result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = (-start_logits[0]).argsort() # here, we are taking just the first position for example.\n",
    "indices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use those indices in the original array, we get this result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_logits[0][indices]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the start_logits showing in descending order!\n",
    "\n",
    "\n",
    "Now, let's really transform the logits into string and you'll understand everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_largest = 20 # Number of start and end logits we want to search\n",
    "max_answer_length = 30 # Max answer length we want to allow\n",
    "predict_answers = [] # List of predicted answers will be stored \n",
    "\n",
    "for sample in small_validation_dataset: # For each sample in the NON-processed (it is not tokenized!) small validation dataset (CTRL+click if you want to remember)\n",
    "    sample_id = sample[\"id\"] # Get the id from this sample\n",
    "    context = sample[\"context\"] # and the context\n",
    "\n",
    "    # Initializing best_score and best_answer (they'll be update in the below looping)\n",
    "    best_score = float(\"-inf\") \n",
    "    best_answer = None\n",
    "\n",
    "    for idx in sample_id2idxs[sample_id]: # For each id in the sample_id2idxs (samples here are tokenized!) in the sample_id as index (remebmer it is a dict)\n",
    "        # Grabbing the start and end logits for this index\n",
    "        start_logit = start_logits[idx] \n",
    "        end_logit = end_logits[idx]\n",
    "        # And also get the offset mapping for this index\n",
    "        offsets = small_validation_processed[idx][\"offset_mapping\"] # note that this offset mapping is the processed, containg None for any position\n",
    "                                                                    # that is not in the context\n",
    "        # Sorting the logits as we saw                                                           \n",
    "        start_indices = (-start_logit).argsort() \n",
    "        end_indices = (-end_logit).argsort()\n",
    "\n",
    "        # Next step is to loop through the n_largest start and end logits\n",
    "        for start_idx in start_indices[:n_largest]:\n",
    "            for end_idx in end_indices[:n_largest]:\n",
    "                # Checking the cases where the answer:\n",
    "                if offsets[start_idx] is None or offsets[end_idx] is None: # Answer is not in the context\n",
    "                    continue\n",
    "                if end_idx < start_idx: # Answer does not exist (since is has negative length)\n",
    "                    continue\n",
    "                if (end_idx - start_idx + 1) > max_answer_length: # Answer is longer than allowed\n",
    "                    continue\n",
    "\n",
    "                # If we have an answer,\n",
    "                score = start_logit[start_idx] + end_logit[end_idx] # Compute the score for this answer\n",
    "                if score > best_score: # Checking if score is better than the current best_score\n",
    "                    best_score = score # If yes, compute\n",
    "\n",
    "                    # Getting the position of the first character and of the last character\n",
    "                    first_ch = offsets[start_idx][0] \n",
    "                    last_ch = offsets[end_idx][1]\n",
    "                    # Retrieving the answer as actual text using the them as indices in the context\n",
    "                    best_answer = context[first_ch:last_ch]\n",
    "        # And finally append to the list        \n",
    "        predict_answers.append({\"id\": sample_id, \"prediction_text\": best_answer})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Onde this is done, we just need to format the true answer in the right format for computing the metrcis.\n",
    "\n",
    "To remembem the format, go to the beggining of the \"Metrics and Logits\" outline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_answers = [\n",
    "    {\n",
    "    \"id\": x[\"id\"],\n",
    "    \"answers\": x[\"answers\"]\n",
    "    }\n",
    "    for x in small_validation_dataset\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#true_answers # uncoment to see the result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! We can now turn the logits into string and finally compute the metrics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric.compute(predictions=predict_answers, references=true_answers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's turn the whole process into a function called compute_metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(start_logits, end_logits, processed_dataset, orig_dataset):\n",
    "    sample_id2idxs = {}\n",
    "\n",
    "    for i, id_ in enumerate(processed_dataset[\"sample_id\"]):\n",
    "        if id_ not in sample_id2idxs:\n",
    "            sample_id2idxs[id_] = [i]\n",
    "        else:\n",
    "            sample_id2idxs[id_].append(i)\n",
    "\n",
    "    predicted_answers = []\n",
    "    for sample in tqdm(orig_dataset):\n",
    "\n",
    "        sample_id = sample[\"id\"]\n",
    "        context = sample['context']\n",
    "\n",
    "        best_score = float(\"-inf\")\n",
    "        best_answer = None\n",
    "\n",
    "        for idx in sample_id2idxs[sample_id]:\n",
    "            start_logit = start_logits[idx]\n",
    "            end_logit = end_logits[idx]\n",
    "\n",
    "            offsets = processed_dataset[idx][\"offset_mapping\"]\n",
    "\n",
    "            start_indices = (-start_logit).argsort()\n",
    "            end_indices = (-end_logit).argsort()\n",
    "\n",
    "            for start_idx in start_indices[:n_largest]:\n",
    "                for end_idx in end_indices[:n_largest]:\n",
    "                    if offsets[start_idx] is None or offsets[end_idx] is None:\n",
    "                        continue\n",
    "\n",
    "                    if end_idx < start_idx:\n",
    "                        continue\n",
    "\n",
    "                    if (end_idx - start_idx + 1) > max_answer_length:\n",
    "                        continue\n",
    "\n",
    "                    score = start_logit[start_idx] + end_logit[end_idx]\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "\n",
    "                        first_ch = offsets[start_idx][0] \n",
    "                        last_ch = offsets[end_idx][1]\n",
    "                        best_answer = context[first_ch:last_ch]\n",
    "                \n",
    "        predicted_answers.append({\"id\": sample_id, \"prediction_text\": best_answer})\n",
    "    true_answers = [{\"id\": x[\"id\"], \"answers\": x[\"answers\"]} for x in orig_dataset]\n",
    "    y = metric.compute(predictions=predicted_answers, references=true_answers)\n",
    "    return y\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the function on the small datasets we used earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics(\n",
    "    start_logits,\n",
    "    end_logits,\n",
    "    small_validation_processed,\n",
    "    small_validation_dataset\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!\n",
    "This function will be used after our training step is done!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()\n",
    "mlflow.set_experiment(\"BERT Q&A - distilbert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint_bbc) # Loading the model we want to fine-tune (distilbert-base-cased)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to create our TrainingArguments object with all the necssary arguments for the training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    \"finetuned-squad\", # this is a default name for this model and task\n",
    "    evaluation_strategy=\"no\", # No, because we'll compute metrics manually\n",
    "    save_strategy=\"epoch\", # saving for each step (you can use epoch as well)\n",
    "    learning_rate=2e-5, # learnin rate value \n",
    "    num_train_epochs=3, # 3 epoch in total (max is 4 since out inputs are very large, more tha that is not recommended)\n",
    "    weight_decay=0.01, # regularization technique\n",
    "    fp16=True # speed up the process\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's instantiate a trainer object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model, # our model\n",
    "    args=args, # our args\n",
    "    train_dataset=train_dataset, # our datasets\n",
    "    eval_dataset=validation_dataset,\n",
    "    tokenizer=tokenizer # and our tokenizer\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if the GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time has come! Let's train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()\n",
    "start_time_training = datetime.now() # this is for computing the time it take for the training\n",
    "with mlflow.start_run():\n",
    "    trainer.train() \n",
    "print(f'Total time for training: {datetime.now() - start_time_training}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_prediction = trainer.predict(validation_dataset) # getting the predictions for the validation set\n",
    "trainer_prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And grab just the prediction values from this objetc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, _, _ = trainer_prediction\n",
    "predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a tuple with two arrays, the start_logits and end_logits!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_logits, end_logits = predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Computing the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics(\n",
    "    start_logits,\n",
    "    end_logits,\n",
    "    validation_dataset,\n",
    "    squad_dataset['validation']\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the model for further usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('distilbert_bertqa')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a question-answering pipeline from transformers and pass our model to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = pipeline(\n",
    "    'question-answering',\n",
    "    model='distilbert_bertqa',\n",
    "    device=0 #GPU\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"Tomorrow the Atl√¢ntico is going to have a delicious team lunch!\"\n",
    "question = \"What did the Atl√¢ntico is going to have tomorrow?\"\n",
    "qa(context=context, question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f' {datetime.now() - start_time_all_execution}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
