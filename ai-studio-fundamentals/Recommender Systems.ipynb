{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Advanced Recommender Systems with Python\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg import svds\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import datetime\n",
    "import mlflow\n",
    "from mlflow.types.schema import Schema, ColSpec\n",
    "from mlflow.models import ModelSignature\n",
    "import os\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings('ignore')\n",
    "asset_folder = \"/home/jovyan/datafabric/tutorial/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [MovieLens100K](https://files.grouplens.org/datasets/movielens/ml-100k-README.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `u.data:` dataset containing user interactions with items that identifies movies.\n",
    "**Columns** \n",
    "\n",
    "- `user_id:` A unique identifier for each user.\n",
    "\n",
    "- `item_id:` A unique identifier for each item (a movie, in this case).\n",
    "\n",
    "- `rating:` The rating given by the user to the item. It is usually on a scale (e.g., 1 to 5).\n",
    "\n",
    "- `timestamp:` The timestamp indicating when the rating was given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "column_names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "df = pd.read_csv(f\"{asset_folder}ml-100k/u.data\", sep='\\t', names=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note we only have the item_id, not the movie name. We can use the `Movie_ID_Titles.data` file to grab the movie names and merge into a unique DataFrame:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Movie_Id_Titles:` dataset containing movie titles corresponding to the item IDs in the `u.data` file. \n",
    "\n",
    "**Columns** \n",
    "\n",
    "- `item_id:` A unique identifier for each movie.\n",
    "\n",
    "- `title:` The title of the movie, including its release year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "movie_titles = pd.read_csv(f\"{asset_folder}Movie_Id_Titles.csv\")\n",
    "movie_titles.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then merge the dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.merge(df,movie_titles,on='item_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Final DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.sample(2))\n",
    "display(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have a total of 100003 rows and 5 columns in our DataFrame. Let's explore it! ðŸ˜Ž"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis ðŸ“Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "n_users = df.user_id.nunique()\n",
    "n_items = df.item_id.nunique()\n",
    "\n",
    "print(f'The dataset contains {n_users} users and {n_items} different movies.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_stats = df['rating'].describe().drop('count')\n",
    "plt.figure(figsize=(10, 5))\n",
    "rating_stats.plot(kind='bar', color='orange')\n",
    "plt.title('Rating Statistics')\n",
    "plt.xlabel('Statistic')\n",
    "plt.ylabel('Value')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_ratings_count = df.groupby('title')['rating'].count().sort_values(ascending=False)\n",
    "top_5_movies = movie_ratings_count.head(5)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "top_5_movies.plot(kind='bar', color='purple')\n",
    "plt.title('Top 5 Most Rated Movies')\n",
    "plt.xlabel('Movie Title')\n",
    "plt.ylabel('Number of Ratings')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5_ratings = df[df['rating'] == 5]\n",
    "movies_5_ratings = df_5_ratings.groupby('title')['rating'].count().sort_values(ascending=False).head(5)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "movies_5_ratings.plot(kind='bar', color='green')\n",
    "plt.title('Top 5 Movies with Highest Rating')\n",
    "plt.xlabel('Movie Title')\n",
    "plt.ylabel('Number of 5 Ratings')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling using Memory-Based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Collaborative Filtering?\n",
    "Collaborative filtering is a method used to predict what a user might like by collecting preferences from many users. The basic idea is that if users agree in the past, they will agree in the future.\n",
    "\n",
    "\n",
    "### Memory-Based Collaborative Filtering\n",
    "In memory-based collaborative filtering, the algorithm makes predictions based on the entire user-item dataset stored in memory. It directly uses the historical data to find patterns.\n",
    "\n",
    "In our recommendation system experiment, we are evaluating the performance of three different collaborative filtering techniques: User-User Collaborative Filtering, Item-Item Collaborative Filtering, and SVD-based Collaborative Filtering.\n",
    "\n",
    "- User-User Collaborative Filtering: We calculate the RMSE (Root Mean Square Error) for predictions made by finding users similar to the target user and using their ratings to predict new items.\n",
    "\n",
    "- Item-Item Collaborative Filtering: We calculate the RMSE for predictions made by finding items similar to the ones the target user has liked and using those similarities to make new recommendations.\n",
    "\n",
    "- SVD-based Collaborative Filtering: We use Singular Value Decomposition (SVD) to factorize the user-item interaction matrix and predict missing values. The RMSE of these predictions is also calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split\n",
    "\n",
    "Recommendation Systems by their very nature are very difficult to evaluate, but we will still show you how to evaluate them in this tutorial. In order to do this, we'll split our data into two sets. However, we won't do our classic X_train,X_test,y_train,y_test split. Instead we can actually just segement the data into two sets of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(df, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory-Based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Create two user-item matrices, one for training and another for testing\n",
    "train_data_matrix = np.zeros((n_users, n_items))\n",
    "for line in train_data.itertuples():\n",
    "    train_data_matrix[line[1]-1, line[2]-1] = line[3]  \n",
    "\n",
    "test_data_matrix = np.zeros((n_users, n_items))\n",
    "for line in test_data.itertuples():\n",
    "    test_data_matrix[line[1]-1, line[2]-1] = line[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the [pairwise_distances](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.pairwise_distances.html) function from sklearn to calculate the cosine similarity. Note, the output will range from 0 to 1 since the ratings are all positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "user_similarity = pairwise_distances(train_data_matrix, metric='cosine')\n",
    "item_similarity = pairwise_distances(train_data_matrix.T, metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(ratings, similarity, type='user'):\n",
    "    if type == 'user':\n",
    "        mean_user_rating = ratings.mean(axis=1)\n",
    "        #You use np.newaxis so that mean_user_rating has same format as ratings\n",
    "        ratings_diff = (ratings - mean_user_rating[:, np.newaxis]) \n",
    "        pred = mean_user_rating[:, np.newaxis] + similarity.dot(ratings_diff) / np.array([np.abs(similarity).sum(axis=1)]).T\n",
    "    elif type == 'item':\n",
    "        pred = ratings.dot(similarity) / np.array([np.abs(similarity).sum(axis=1)])     \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "item_prediction = predict(train_data_matrix, item_similarity, type='item')\n",
    "user_prediction = predict(train_data_matrix, user_similarity, type='user')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you only want to consider predicted ratings that are in the test dataset, you filter out all other elements in the prediction matrix with `prediction[ground_truth.nonzero()]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(prediction, ground_truth):\n",
    "    prediction = prediction[ground_truth.nonzero()].flatten() \n",
    "    ground_truth = ground_truth[ground_truth.nonzero()].flatten()\n",
    "    return sqrt(mean_squared_error(prediction, ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print('User-based CF RMSE: ' + str(rmse(user_prediction, test_data_matrix)))\n",
    "print('Item-based CF RMSE: ' + str(rmse(item_prediction, test_data_matrix)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sparsity=round(1.0-len(df)/float(n_users*n_items),3)\n",
    "print('The sparsity level is ' +  str(sparsity*100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#get SVD components from train matrix. Choose k.\n",
    "u, s, vt = svds(train_data_matrix, k = 20)\n",
    "s_diag_matrix=np.diag(s)\n",
    "X_pred = np.dot(np.dot(u, s_diag_matrix), vt)\n",
    "print('User-based CF MSE: ' + str(rmse(X_pred, test_data_matrix)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard + Mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"/phoenix/tensorboard/tensorlogs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True, write_images=True)\n",
    "\n",
    "mlflow.set_experiment(\"MovieRecommenderExperiment\")\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    user_rmse = rmse(user_prediction, test_data_matrix)\n",
    "    item_rmse = rmse(item_prediction, test_data_matrix)\n",
    "    svd_rmse = rmse(X_pred, test_data_matrix)\n",
    "    \n",
    "    mlflow.log_metric(\"User_based_CF_RMSE\", user_rmse)\n",
    "    mlflow.log_metric(\"Item_based_CF_RMSE\", item_rmse)\n",
    "    mlflow.log_metric(\"User_based_CF_MSE_SVD\", svd_rmse)\n",
    "    \n",
    "    writer = tf.summary.create_file_writer(log_dir)\n",
    "    \n",
    "    with writer.as_default():\n",
    "        tf.summary.scalar(\"User_based_CF_RMSE\", user_rmse, step=1)\n",
    "        tf.summary.scalar(\"Item_based_CF_RMSE\", item_rmse, step=1)\n",
    "        tf.summary.scalar(\"User_based_CF_MSE_SVD\", svd_rmse, step=1)\n",
    "        writer.flush()\n",
    "\n",
    "    mlflow.tensorflow.autolog()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLflow + Model Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_ratings(ratings, min_rating=1, max_rating=5):\n",
    "    ratings = np.array(ratings)\n",
    "    if ratings.max() == ratings.min():\n",
    "        return np.full(ratings.shape, (max_rating + min_rating) / 2)\n",
    "    normalized_ratings = (ratings - ratings.min()) / (ratings.max() - ratings.min()) * (max_rating - min_rating) + min_rating\n",
    "    return np.clip(normalized_ratings, min_rating, max_rating)  \n",
    "\n",
    "class MovieRecommender(mlflow.pyfunc.PythonModel):\n",
    "\n",
    "    def load_context(self, context):\n",
    "        self.train_data_matrix = np.load(context.artifacts[\"train_data_matrix\"])\n",
    "        self.movie_titles = pd.read_csv(context.artifacts[\"movie_titles_path\"])\n",
    "        self.n_users, self.n_items = self.train_data_matrix.shape\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        movie_id = int(model_input['movie_id'][0])\n",
    "        rating = float(model_input['rating'][0])\n",
    "\n",
    "        user_ratings = np.zeros(self.n_items)\n",
    "        user_ratings[movie_id - 1] = rating\n",
    "\n",
    "        ratings = np.vstack([self.train_data_matrix, user_ratings])\n",
    "\n",
    "        item_similarity = pairwise_distances(ratings.T, metric='cosine')\n",
    "\n",
    "        mean_item_rating = ratings[:-1].mean(axis=0)  \n",
    "        ratings_diff = (ratings[:-1] - mean_item_rating)\n",
    "        pred = mean_item_rating + item_similarity.dot(ratings_diff.T).T[-1]\n",
    "\n",
    "        user_pred_normalized = normalize_ratings(pred)\n",
    "\n",
    "        movie_titles = self.movie_titles['title'].tolist()\n",
    "\n",
    "        predictions_with_titles = list(zip(movie_titles, user_pred_normalized))\n",
    "\n",
    "        print(\"Original predictions:\", pred)\n",
    "        print(\"Normalized predictions:\", user_pred_normalized)\n",
    "\n",
    "        return predictions_with_titles\n",
    "\n",
    "    @classmethod\n",
    "    def log_model(cls, train_data_matrix_path, movie_titles_path):\n",
    "        input_schema = Schema([\n",
    "            ColSpec(\"long\", \"movie_id\"),\n",
    "            ColSpec(\"double\", \"rating\")\n",
    "        ])\n",
    "        output_schema = Schema([\n",
    "            ColSpec(\"string\", \"movie_title\"),\n",
    "            ColSpec(\"double\", \"prediction\")\n",
    "        ])\n",
    "        signature = ModelSignature(inputs=input_schema, outputs=output_schema)\n",
    "\n",
    "        mlflow.pyfunc.log_model(\n",
    "            artifact_path=\"movie_recommender_model\",\n",
    "            python_model=cls(),\n",
    "            artifacts={\n",
    "                \"train_data_matrix\": train_data_matrix_path,\n",
    "                \"movie_titles_path\": movie_titles_path\n",
    "            },\n",
    "            signature=signature,\n",
    "            pip_requirements=[\"mlflow\", \"pandas\", \"scikit-learn\", \"numpy\"]\n",
    "        )\n",
    "\n",
    "output_dir = \"model_artifacts\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "train_data_matrix_path = os.path.join(output_dir, \"train_data_matrix.npy\")\n",
    "np.save(train_data_matrix_path, train_data_matrix)\n",
    "movie_titles_path = os.path.join(output_dir, \"movie_titles.csv\")\n",
    "movie_titles.to_csv(movie_titles_path, index=False)\n",
    "\n",
    "mlflow.set_experiment(\"MovieRecommenderExperiment\")\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    MovieRecommender.log_model(train_data_matrix_path, movie_titles_path)\n",
    "    model_uri = f\"runs:/{run.info.run_id}/movie_recommender_model\"\n",
    "    mlflow.register_model(model_uri=model_uri, name=\"MovieRecommenderModel\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
